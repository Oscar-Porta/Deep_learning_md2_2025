# -*- coding: utf-8 -*-
"""Ej5_finanzas.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TlI2LLq_4mXtL3k-YaJ0RHFBe9kUTwzm

# Ejercicio 5 ‚Äî Efecto de caracter√≠sticas estructurales sobre la rentabilidad operativa

**Objetivo**  
Analizar el efecto de caracter√≠sticas estructurales (pa√≠s, sector, tama√±o, a√±o) sobre `EBITDA_Ingresos_netos` y construir un modelo **predictivo** (√°rbol de regresi√≥n). Finalmente, estimar el **CATE** del tratamiento *Leverage alto vs bajo* controlando por el resto de variables observables.

**Variables**
- **Target**: `EBITDA_Ingresos_netos`
- **Categ√≥ricas**: `country`, `year`, `sector`, `size`
- **Num√©ricas**: `total_assets`, `Solvencia`, `Leverage`

**√çndice de trabajo**
1. Preparaci√≥n del entorno.
2. Carga y verificaci√≥n de datos
3. An√°lisis descriptivo (EDA): categ√≥ricas vs num√©ricas
4. Modelo lineal con caracter√≠sticas estructurales (country, sector, size, year)
5. Modelo predictivo (√°rbol de regresi√≥n): m√©tricas e importancia
6. Efecto causal de `Leverage` (CATE alto vs bajo) con √°rboles
7. Resultados por subgrupos (pa√≠s, sector, tama√±o)
8. Conclusiones

## 1.Preparaci√≥n del entorno

En este apartado configuramos el entorno de trabajo para que el notebook sea **portable** y pueda ejecutarse tanto en **Google Drive** como en local.  
Para ello utilizamos dos banderas:

- **`USE_DRIVE`**: controla si trabajamos con archivos almacenados en Google Drive (`True`) o en el directorio local (`False`).
- **`RUN_MODELING`**: determina si el notebook debe ejecutar el **procesado y entrenamiento completo** (`True`) o, en modo entrega (`False`), cargar directamente los resultados y artefactos previamente guardados.

Adem√°s:
- Definimos la **estructura de carpetas** para mantener organizados datos, salidas y scripts.
- Creamos un **README** que documenta el contenido y uso de cada carpeta.
- Cargamos **todas las librer√≠as necesarias** en una √∫nica celda.
- Configuramos un **estilo gr√°fico consistente** que usaremos en todos los gr√°ficos.

Con esta preparaci√≥n aseguramos un flujo de trabajo reproducible y ordenado desde el inicio del ejercicio.
"""

#Flags, rutas base y (si aplica) montaje de Drive

# === Flags ===
USE_DRIVE    = False   # Trabajo habitual en Drive (entrega ‚Üí False)
RUN_MODELING = False   # Generar artefactos (entrega ‚Üí False, solo cargar)

from pathlib import Path

# === Definici√≥n de rutas base ===
if USE_DRIVE:
    try:
        from google.colab import drive
        drive.mount('/content/drive', force_remount=False)
        ROOT = Path("/content/drive/MyDrive/MASTER BIG DATA/md2_2025")
    except Exception as e:
        print("No se pudo montar Drive. Trabajaremos en local.")
        USE_DRIVE = False

if not USE_DRIVE:
    ROOT = Path(".").resolve()

# Carpeta del ejercicio
EJ5 = ROOT / "ej5_finanzas_causal_predictivo"

# Diccionario de rutas
PATHS = {
    "ej":          str(EJ5),
    "nb":          str(EJ5 / "notebooks"),
    "src":         str(EJ5 / "src"),
    "data_raw":    str(EJ5 / "data" / "raw"),
    "data_proc":   str(EJ5 / "data" / "processed"),
    "out_csv":     str(EJ5 / "outputs" / "csv"),
    "out_figs":    str(EJ5 / "outputs" / "figures"),
    "out_models":  str(EJ5 / "outputs" / "models"),
    "docs":        str(EJ5 / "docs"),
    "scripts":     str(EJ5 / "scripts"),
}

print("Flags ‚Üí USE_DRIVE =", USE_DRIVE, "| RUN_MODELING =", RUN_MODELING)
print("Ra√≠z de trabajo:", ROOT)
print("Carpeta del ejercicio:", EJ5)

# 2) Estructura de carpetas y README del ejercicio
def ensure_dir(p: str):
    Path(p).mkdir(parents=True, exist_ok=True)

# Crear estructura
for k in ("nb","src","data_raw","data_proc","out_csv","out_figs","out_models","docs","scripts"):
    ensure_dir(PATHS[k])

# Crear README con descripci√≥n
readme_path = Path(PATHS["ej"]) / "README.md"
readme_text = """# Ejercicio 5 ‚Äî Efecto de caracter√≠sticas estructurales sobre la rentabilidad operativa

Este proyecto analiza el impacto de variables estructurales (pa√≠s, sector, tama√±o, a√±o) sobre la rentabilidad operativa (`EBITDA_Ingresos_netos`),
construye un modelo predictivo (√°rbol de regresi√≥n) y estima el efecto causal (CATE) de `Leverage alto vs bajo`, controlando por observables.

**Notebook principal:** notebooks/Ej5_finanzas.ipynb, abrelo para ejecutar el flujo completo del ejercicio siguiendo las instrucciones.
**Datos (no incluidos):** coloca el fichero de datos en `data/raw/` (CSV o Excel).
**Modo trabajo:** `USE_DRIVE=True`, `RUN_MODELING=True` para generar artefactos.
**Modo entrega:** `USE_DRIVE=False`, `RUN_MODELING=False` para cargar artefactos desde `outputs/`.

**Estructura:**
- `data/raw/`       ‚Üí datos originales
- `data/processed/` ‚Üí datos limpios / transformados
- `outputs/csv/`    ‚Üí tablas de resultados (m√©tricas, importancias, CATE, etc.)
- `outputs/figures/`‚Üí gr√°ficos (EDA, √°rbol, subgrupos)
- `outputs/models/` ‚Üí modelos guardados (√°rbol, preprocesamiento)
- `docs/`           ‚Üí documentaci√≥n auxiliar

**Notas:**
- El √°rbol predictivo usar√° `country`, `year`, `sector`, `size`, `total_assets`, `Solvencia`, `Leverage`.
- Para CATE definiremos `Leverage alto` vs `bajo` y estimaremos efectos heterog√©neos por subgrupos.
"""
if not readme_path.exists():
    readme_path.write_text(readme_text, encoding="utf-8")
    print("README creado en:", readme_path)
else:
    print("README ya exist√≠a en:", readme_path)

print("Estructura verificada. Directorios clave:")
for k in ("data_raw","data_proc","out_csv","out_figs","out_models"):
    print(f"  {k:>10} ‚Üí {PATHS[k]}")

# 3) Imports √∫nicos y estilo gr√°fico
import os, json, warnings, random
from pathlib import Path

import numpy as np
import pandas as pd

import matplotlib.pyplot as plt
import seaborn as sns

# Modelado y evaluaci√≥n
from sklearn.model_selection import train_test_split, KFold, cross_val_score
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.tree import DecisionTreeRegressor, plot_tree
from sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error
from sklearn.model_selection import GroupKFold, RandomizedSearchCV
from scipy.stats.mstats import winsorize
from scipy.stats import randint

# Lineal (explicativo)
import statsmodels.api as sm
import statsmodels.formula.api as smf
from statsmodels.formula.api import ols

# Persistencia
import joblib

# Estilo gr√°fico consistente
sns.set_theme(style='whitegrid', palette='Set2', context='talk')

# Aleatoriedad controlada
SEED = 42
np.random.seed(SEED)
random.seed(SEED)

# Opciones de impresi√≥n
pd.set_option('display.max_columns', 120)
pd.set_option('display.width', 120)

print("Librer√≠as cargadas. SEED =", SEED)

"""## 2.Carga y limpieza inicial de datos

En este paso leemos el fichero `empresas.csv` (separador `;`), normalizamos y renombramos **todas las columnas al ingl√©s** siguiendo un formato consistente en min√∫sculas y `snake_case`.  
Esto garantiza que todo el flujo de trabajo utilice nombres homog√©neos, evitando mezclas de idiomas o formatos.

Adem√°s:
- Verificamos tipos de variables, distinguiendo entre **categ√≥ricas** y **num√©ricas**.
- Convertimos correctamente `year` a entero y magnitudes/ratios a num√©rico.
- Guardamos una versi√≥n limpia y estable en `data/processed/empresas_limpio.csv`.

**Entrada**: `data/raw/empresas.csv`  
**Salida**: `data/processed/empresas_limpio.csv`  
**Objetivo**: disponer de un dataset coherente para el EDA, el modelo lineal, el √°rbol de regresi√≥n y la estimaci√≥n causal (CATE).
"""

# 2) Carga y limpieza inicial de datos

import unicodedata

RAW_PATH = Path(PATHS["data_raw"]) / "empresas.csv"
PROC_PATH = Path(PATHS["data_proc"]) / "empresas_limpio.csv"

def _normalize_colname(s: str) -> str:
    # min√∫sculas, sin tildes, reemplazo de espacios y s√≠mbolos por "_"
    s = s.strip().lower()
    s = ''.join(c for c in unicodedata.normalize('NFKD', s) if not unicodedata.combining(c))
    for ch in [' ', '-', '/', '\\', '(', ')', '.', ',', '%']:
        s = s.replace(ch, '_')
    s = s.replace('__', '_')
    return s

# 1) Lectura con separador correcto
df_raw = pd.read_csv(RAW_PATH, sep=';', dtype=str)

# 2) Normalizar nombres base
df_raw.columns = [_normalize_colname(c) for c in df_raw.columns]

# 3) Renombrado completo a ingl√©s
rename_map = {
   'pais': 'country',
    'anio': 'year',
    'sector': 'sector',
    'tamanio': 'size',
    'total_activos': 'total_assets',
    'facturacion': 'revenue',
    'valor_bruto_agregado': 'gross_value_added',
    'trabajadores': 'num_employees',
    'solvencia': 'solvency',
    'apalancamiento': 'leverage',
    'liquidez': 'liquidity',
    'ebitda_ingresos_netos': 'ebitda_net_income',
    'beneficio_econ_ingresos_netos': 'economic_profit_net_income',
    'ebit_ingresos_netos': 'ebit_net_income',
    'rotacion_activos': 'asset_turnover',
    'salarios_va': 'wages_gva'
}

# Aplicar renombrado para las columnas presentes
df_raw = df_raw.rename(columns={c: rename_map.get(c, c) for c in df_raw.columns})

# 4) Definir categ√≥ricas y num√©ricas
cat_cols = [c for c in ['country', 'year', 'sector', 'size'] if c in df_raw.columns]
num_cols_candidates = [
    'total_assets', 'revenue', 'gross_value_added', 'num_firms', 'num_employees',
    'ebitda_net_income', 'economic_profit_net_income', 'ebit_net_income',
    'asset_turnover', 'wages_gva', 'solvency', 'leverage', 'liquidity'
]
num_cols = [c for c in num_cols_candidates if c in df_raw.columns]

# Convertir year a entero
if 'year' in df_raw.columns:
    df_raw['year'] = pd.to_numeric(df_raw['year'], errors='coerce').astype('Int64')

# Convertir num√©ricas
for c in num_cols:
    df_raw[c] = (df_raw[c]
                 .str.replace(',', '.', regex=False)
                 .str.replace(' ', '', regex=False)
                 .str.replace('%', '', regex=False)
                 )
    df_raw[c] = pd.to_numeric(df_raw[c], errors='coerce')

# Limpiar espacios en categ√≥ricas
for c in cat_cols:
    df_raw[c] = df_raw[c].astype(str).str.strip()

# 5) Reporte r√°pido
print("Columnas despu√©s de limpieza y renombrado:")
print(df_raw.columns.tolist())
print("\nTipos inferidos:")
print(df_raw[cat_cols + num_cols].dtypes if (cat_cols or num_cols) else df_raw.dtypes)

print("\nResumen de nulos en variables clave:")
keys = list(dict.fromkeys(['ebitda_net_income', 'solvency', 'leverage', 'total_assets'] + cat_cols))
keys = [k for k in keys if k in df_raw.columns]
print(df_raw[keys].isna().sum().sort_values(ascending=False).head(12))

# 6) Guardar procesado
df_raw.to_csv(PROC_PATH, index=False, encoding='utf-8')
print("\n‚úÖ Dataset limpio guardado en:", PROC_PATH)

# Nota: En RUN_MODELING=False cargaremos directamente PROC_PATH.

"""## 3.An√°lisis Descriptivo (EDA)

En este apartado realizamos un an√°lisis exploratorio de los datos, combinando para cada tipo de variable (categ√≥rica y num√©rica) tanto un resumen tabular como visualizaciones gr√°ficas.  
El objetivo es identificar patrones generales, valores at√≠picos y posibles problemas de calidad de datos que puedan influir en los modelos posteriores.

### 3.1Variables categ√≥ricas

Para las variables categ√≥ricas (`country`, `year`, `sector`, `size`) mostraremos:

- **Tablas de frecuencia**: n√∫mero absoluto de registros y porcentaje de cada categor√≠a.
- **Gr√°ficos de barras**: frecuencia por categor√≠a, ordenadas de mayor a menor, para identificar categor√≠as dominantes o poco representadas.
"""

# 3.1 EDA - Variables categ√≥ricas
# Cargar dataset limpio
df = pd.read_csv(Path(PATHS["data_proc"]) / "empresas_limpio.csv")

# Definir variables categ√≥ricas
cat_vars = ['country', 'year', 'sector', 'size']
cat_vars = [c for c in cat_vars if c in df.columns]

# Tabla de frecuencias por variable categ√≥rica
for col in cat_vars:
    freq_table = df[col].value_counts(dropna=False).reset_index()
    freq_table.columns = [col, 'count']
    freq_table['percentage'] = 100 * freq_table['count'] / len(df)
    print(f"\nüìä Frecuencia para {col}:")
    print(freq_table)

# Gr√°ficos en grid
sns.set_theme(style='whitegrid', palette='Set2', context='talk')
fig, axes = plt.subplots(nrows=1, ncols=len(cat_vars), figsize=(6*len(cat_vars), 5))

if len(cat_vars) == 1:
    axes = [axes]  # Para iterar si solo hay 1 variable

for ax, col in zip(axes, cat_vars):
    order = df[col].value_counts().index
    sns.countplot(data=df, x=col, order=order, ax=ax)
    ax.set_title(f"Frecuencia - {col}")
    ax.set_xlabel("")
    ax.set_ylabel("Count")
    ax.tick_params(axis='x', rotation=45)

plt.tight_layout()
plt.show()

"""### 3.2Variables num√©ricas

Para las variables num√©ricas (`total_assets`, `revenue`, `gross_value_added`, `num_employees`, `solvency`, `leverage`, `liquidity`, `ebitda_net_income`, `economic_profit_net_income`, `ebit_net_income`, `asset_turnover`, `wages_gva`) mostraremos:

- **Tabla de estad√≠sticos descriptivos**: recuento, media, desviaci√≥n est√°ndar, m√≠nimo, m√°ximo y percentiles 25, 50 y 75.
- **Visualizaciones combinadas**:
  - **Histogramas**: para observar la distribuci√≥n de los valores.
  - **Boxplots**: para identificar valores at√≠picos.
  - Ambos gr√°ficos se presentar√°n en un mismo grid para facilitar la comparaci√≥n.

El an√°lisis descriptivo nos permitir√° establecer una primera impresi√≥n sobre la dispersi√≥n, simetr√≠a y posibles sesgos en los datos, as√≠ como detectar categor√≠as con muy baja frecuencia o valores extremos.
"""

# 3.2 EDA - Variables num√©ricas

# Definir variables num√©ricas
num_vars = [
    'total_assets', 'revenue', 'gross_value_added', 'num_employees',
    'solvency', 'leverage', 'liquidity', 'ebitda_net_income',
    'economic_profit_net_income', 'ebit_net_income', 'asset_turnover', 'wages_gva'
]
num_vars = [c for c in num_vars if c in df.columns]

# Tabla de estad√≠sticos descriptivos
desc_table = df[num_vars].describe().transpose()
print("üìä Estad√≠sticos descriptivos de variables num√©ricas:")
print(desc_table)

# Gr√°ficos combinados (histograma + boxplot por variable)
sns.set_theme(style='whitegrid', palette='Set2', context='talk')

fig, axes = plt.subplots(nrows=len(num_vars), ncols=2, figsize=(12, 4*len(num_vars)))

for i, col in enumerate(num_vars):
    # Histograma
    sns.histplot(data=df, x=col, kde=True, ax=axes[i,0], color='tab:blue')
    axes[i,0].set_title(f"Histograma - {col}")

    # Boxplot
    sns.boxplot(data=df, x=col, ax=axes[i,1], color='tab:orange')
    axes[i,1].set_title(f"Boxplot - {col}")

plt.tight_layout()
plt.show()

"""El conjunto de datos presenta un equilibrio notable en las variables categ√≥ricas:  
- **Pa√≠s**: distribuci√≥n exactamente 50%-50% entre Espa√±a y Francia.  
- **A√±o**: valores uniformemente distribuidos entre 2000 y 2019 (5% cada a√±o).  
- **Sector**: tres sectores representados de forma id√©ntica (33,3% cada uno).  
- **Tama√±o de empresa**: tres niveles con igual proporci√≥n (33,3% cada uno).

En las variables num√©ricas se observan magnitudes muy heterog√©neas:  
- Los activos totales (`total_assets`) y la cifra de negocio (`revenue`) muestran alta dispersi√≥n y valores m√°ximos muy superiores a la media, lo que indica posible asimetr√≠a.  
- Ratios como `solvency` y `leverage` presentan valores extremos que podr√≠an influir en los modelos y requerir transformaciones.  
- La variable objetivo `ebitda_net_income` var√≠a entre 2,32 y 17,56, con media cercana a 6,92.  

En conjunto, el dataset est√° limpio y equilibrado en lo categ√≥rico, pero con variables num√©ricas que presentan escalas y dispersi√≥n muy distintas, lo que deber√° considerarse en la modelizaci√≥n.

## 4.Modelo lineal con caracter√≠sticas estructurales

En este apartado ajustamos un modelo de regresi√≥n lineal m√∫ltiple mediante el m√©todo de **M√≠nimos Cuadrados Ordinarios (OLS)** para analizar el efecto de las variables estructurales sobre la rentabilidad operativa (`ebitda_net_income`).  
Las variables estructurales incluidas como explicativas son:

- **`country`**: pa√≠s en el que opera la empresa.  
- **`sector`**: sector de actividad.  
- **`size`**: tama√±o de la empresa.  
- **`year`**: a√±o de referencia.

### Objetivo
Determinar c√≥mo var√≠a el ratio `ebitda_net_income` seg√∫n el pa√≠s, sector, tama√±o y a√±o, controlando simult√°neamente por todas ellas.

### Metodolog√≠a
1. Codificamos las variables categ√≥ricas en formato *dummy* para poder incluirlas en el modelo.  
2. Ajustamos un modelo de **M√≠nimos Cuadrados Ordinarios (OLS)** usando `statsmodels`.  
3. Interpretamos los coeficientes en relaci√≥n con una **categor√≠a de referencia** para cada variable.  
4. Evaluamos significancia estad√≠stica de los efectos.

**Nota**: Este modelo no pretende predecir, sino estimar **efectos medios** de las caracter√≠sticas estructurales sobre la rentabilidad.
"""

# --- Carga del dataset limpio ---
PROC_PATH = Path(PATHS["data_proc"]) / "empresas_limpio.csv"
dfm = pd.read_csv(PROC_PATH)

# --- Definir target y variables categ√≥ricas ---
y = "ebitda_net_income"
X_cats = ["country", "sector", "size", "year"]

# --- Asegurar que las categ√≥ricas son string para statsmodels ---
for c in X_cats:
    if c in dfm.columns:
        dfm[c] = dfm[c].astype(str)

# --- Filtrar columnas necesarias y quitar nulos ---
cols_modelo = [y] + X_cats
dfm = dfm[cols_modelo].dropna()
print(f"Observaciones usadas en OLS: {len(dfm)}")

# --- Ajuste OLS: y ~ C(country)+C(sector)+C(size)+C(year) ---
formula = f"{y} ~ " + " + ".join([f"C({c})" for c in X_cats])
model = ols(formula=formula, data=dfm).fit()

# --- Resumen por pantalla ---
print(model.summary())

"""#### Comparaci√≥n con errores est√°ndar robustos (HC3)

Tras ajustar el modelo de regresi√≥n lineal m√∫ltiple mediante M√≠nimos Cuadrados Ordinarios (OLS) en el paso anterior, decidimos recalcular los errores est√°ndar utilizando el estimador robusto HC3.

La motivaci√≥n principal es que, en la pr√°ctica, la homocedasticidad de los residuos rara vez se cumple en datos reales. La presencia de heterocedasticidad provoca que los errores est√°ndar cl√°sicos est√©n mal estimados, afectando a los valores t y p, y con ello a las conclusiones sobre la significancia estad√≠stica de los coeficientes.
"""

# --- OLS con errores robustos HC3 ---
robust_hc3 = model.get_robustcov_results(cov_type='HC3')

# --- Resumen por pantalla ---
print(robust_hc3.summary())

"""#### Conclusiones del ajuste con errores robustos

La comparaci√≥n entre el modelo OLS cl√°sico y el ajustado con errores robustos HC3 muestra que:

- **Los coeficientes estimados son id√©nticos** en ambos modelos, ya que el m√©todo de M√≠nimos Cuadrados Ordinarios no cambia.
- **Los errores est√°ndar han variado** en varios coeficientes, modificando ligeramente los valores t y los p-valores.
- En este caso, la significancia de las variables clave (por ejemplo, el pa√≠s y el sector G) se mantiene, aunque en algunos a√±os las diferencias en p-valor se acercan m√°s o menos al umbral de significancia.
- Este ajuste confirma que nuestras conclusiones sobre el impacto de las variables estructurales son robustas frente a posibles problemas de heterocedasticidad.

En consecuencia, consideramos m√°s fiable la interpretaci√≥n basada en el modelo OLS con errores HC3, dado que refleja un escenario m√°s realista en la variabilidad de los residuos.

#### **Conclusi√≥n sobre el efecto de las variables estructurales en la rentabilidad operativa**

A partir del modelo de regresi√≥n lineal m√∫ltiple ajustado por M√≠nimos Cuadrados Ordinarios con errores est√°ndar robustos HC3, observamos que:

- **Pa√≠s**: Existe un efecto significativo del pa√≠s sobre la rentabilidad operativa. En particular, las empresas ubicadas en Francia presentan, en promedio, un EBITDA sobre ingresos netos **1,95 puntos porcentuales inferior** al de las empresas espa√±olas, manteniendo constantes el resto de factores.
- **Sector de actividad**: El sector G muestra un efecto negativo importante (-3,53) y altamente significativo, mientras que el sector F no presenta un impacto estad√≠sticamente relevante en comparaci√≥n con el sector de referencia.
- **Tama√±o de la empresa**: Tanto las empresas de tama√±o 2 como las de tama√±o 3 presentan un efecto positivo y significativo sobre la rentabilidad operativa (aprox. +0,80 y +0,84 respectivamente) frente al tama√±o 1.
- **A√±o**: La mayor√≠a de los coeficientes asociados a los distintos a√±os no son estad√≠sticamente significativos, salvo en periodos concretos como 2009-2014, donde se aprecia un descenso notable en la rentabilidad operativa respecto al a√±o base. Esto podr√≠a reflejar coyunturas econ√≥micas adversas en esos ejercicios.

En conjunto, el an√°lisis confirma que la **rentabilidad operativa est√° fuertemente condicionada por factores estructurales**, especialmente el pa√≠s, el sector y el tama√±o de la empresa, mientras que el a√±o s√≥lo influye de forma clara en determinados periodos. Estos resultados, al estar basados en errores robustos, son menos sensibles a problemas de heterocedasticidad, proporcionando inferencias m√°s fiables.

## 5.Modelo de √°rbol de regresi√≥n

En este apartado entrenamos un modelo de **√°rbol de decisi√≥n de regresi√≥n** para estimar el ratio `ebitda_net_income`, utilizando como variables explicativas las categ√≥ricas `country`, `year`, `sector` y `size`, junto con las num√©ricas `total_assets`, `solvency` y `leverage`.  

El modelo se ejecuta en **modo trabajo**, guardando el artefacto para su carga en **modo entrega**, garantizando as√≠ la reproducibilidad.
"""

# === Rutas de artefactos ===
MODEL_DIR = Path(PATHS["out_models"])
MODEL_PATH = MODEL_DIR / "tree_reg_ebitda_time.pkl"

# === Variables ===
y_var = "ebitda_net_income"
cat_vars = ["country", "year", "sector", "size"]
num_vars = ["total_assets", "solvency", "leverage"]

# === Cargar datos procesados ===
df_tree = pd.read_csv(Path(PATHS["data_proc"]) / "empresas_limpio.csv").copy()

# --- Winsorizaci√≥n p1‚Äìp99 en num√©ricas ---
for col in num_vars:
    if col in df_tree.columns:
        w = winsorize(df_tree[col].values, limits=[0.01, 0.01])
        df_tree[col] = np.asarray(w, dtype=float)

# --- One-hot encoding de categ√≥ricas ---
df_tree_enc = pd.get_dummies(df_tree[cat_vars + num_vars], drop_first=True)

# --- Target y matriz de predictores ---
y_data = df_tree[y_var].values
X_data = df_tree_enc.values
feature_names = df_tree_enc.columns

# === Partici√≥n temporal (2000‚Äì2015 train, 2016‚Äì2019 test) ===
train_mask = df_tree["year"] <= 2015
X_train, X_test = X_data[train_mask], X_data[~train_mask]
y_train, y_test = y_data[train_mask], y_data[~train_mask]

# === Entrenar o cargar seg√∫n modo ===
if RUN_MODELING:
    tree_model = DecisionTreeRegressor(
        max_depth=5, min_samples_leaf=20, random_state=42
    )
    tree_model.fit(X_train, y_train)
    joblib.dump({"model": tree_model, "features": feature_names}, MODEL_PATH)
else:
    payload = joblib.load(MODEL_PATH)
    tree_model = payload["model"]
    feature_names = payload["features"]

# === M√©tricas ===
y_pred = tree_model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"RMSE: {rmse:.4f}")
print(f"R¬≤:   {r2:.4f}")

# === Importancia de variables ===
importances = pd.Series(tree_model.feature_importances_, index=feature_names)
importances = importances.sort_values(ascending=False)
print("\nTop-10 importancias:")
print(importances.head(10))

# === Visualizaci√≥n del √°rbol===
plt.figure(figsize=(20, 10))
plot_tree(tree_model, feature_names=feature_names, filled=True, fontsize=7)
plt.show()

"""### Conclusiones del modelo de √°rbol de regresi√≥n

En este apartado hemos desarrollado un modelo predictivo basado en **√°rbol de decisi√≥n de regresi√≥n** para estimar el ratio `ebitda_net_income` a partir de variables estructurales (`country`, `year`, `sector`, `size`, `total_assets`, `solvency` y `leverage`).  

Inicialmente probamos varias configuraciones:
- **√Årbol con partici√≥n estratificada por `year`** (entrenamiento y test con representaci√≥n proporcional de todos los a√±os).  
- **√Årbol con transformaci√≥n logar√≠tmica de `total_assets`** para reducir asimetr√≠a.  

Tras analizar los resultados, decidimos **eliminar estos modelos del informe** para ganar claridad, ya que:
- El modelo con `log(total_assets)` no mostr√≥ mejoras significativas en m√©tricas ni en la relevancia de las variables.
- El √°rbol con partici√≥n estratificada temporalmente no representaba de forma realista un escenario de predicci√≥n a futuro.

Finalmente, realizamos una **winsorizaci√≥n** de las variables num√©ricas al percentil 1‚Äì99 para mitigar el efecto de valores extremos y un **one-hot encoding** para transformar las variables categ√≥ricas en formato num√©rico.   Adem√°s, optamos por una **partici√≥n temporal estricta** (train: 2000‚Äì2015, test: 2016‚Äì2019) para simular un contexto real de predicci√≥n sobre datos futuros, aunque esto penalice el ajuste.  
El modelo resultante obtuvo:
- **RMSE:** 1.5675  
- **R¬≤:** 0.4000  

El modelo ha revelado una fuerte dependencia de ciertas variables categ√≥ricas, mientras que las num√©ricas han tenido un peso menor en la predicci√≥n inicial.  
En el ranking de importancia de variables, destacan:

1. **`sector_G`** como factor principal (**51,8%** de la importancia total).
2. **`country_FR`** (**18,2%**) y **`year`** (**16,8%**) como determinantes secundarios.
3. Entre las num√©ricas, **`solvency`** (**6,0%**) y **`total_assets`** (**0,4%**) aparecen con contribuciones reducidas.
4. Variables como **`size`** y **`leverage`** no han mostrado peso en el modelo.

Para mejorar el rendimiento manteniendo un modelo interpretable, afinamos un **√°rbol de decisi√≥n** con un **RandomizedSearchCV** sobre:
`max_depth`, `min_samples_leaf`, `min_samples_split` y `criterion`.

**Esquema de validaci√≥n**  
- **Partici√≥n temporal fija** para evaluaci√≥n: **train = 2000‚Äì2015**, **test = 2016‚Äì2019**.  
- Dentro del **train**, usamos **GroupKFold por `year`** (sin mezclar a√±os entre folds), evitando fuga temporal durante el tuning.

**Preprocesado**  
- **Clipping p1‚Äìp99** (calculado en train) en `total_assets`, `solvency`, `leverage`.  
- **One-hot encoding** y alineaci√≥n de columnas entre train/test.  

"""

# 5.2 RandomizedSearchCV para DecisionTreeRegressor con validaci√≥n temporal


# --- Configuraci√≥n y datos ---
PROC = Path(PATHS["data_proc"]) / "empresas_limpio.csv"
df_all = pd.read_csv(PROC).copy()

y_var = "ebitda_net_income"
cat_vars = ["country", "year", "sector", "size"]
num_vars = ["total_assets", "solvency", "leverage"]

# Partici√≥n temporal fija
train_mask = df_all["year"] <= 2015
df_train = df_all.loc[train_mask].copy()
df_test  = df_all.loc[~train_mask].copy()

# --- Clipping p1‚Äìp99 SOLO con informaci√≥n de train ---
clip_info = {}
for col in num_vars:
    q_low  = df_train[col].quantile(0.01)
    q_high = df_train[col].quantile(0.99)
    clip_info[col] = (float(q_low), float(q_high))
    df_train[col] = df_train[col].clip(q_low, q_high)
    df_test[col]  = df_test[col].clip(q_low, q_high)

# --- One-hot y alineaci√≥n de columnas ---
X_train = pd.get_dummies(df_train[cat_vars + num_vars], drop_first=True)
X_test  = pd.get_dummies(df_test[cat_vars + num_vars], drop_first=True)
# Alinear columnas de test al espacio de train
X_test = X_test.reindex(columns=X_train.columns, fill_value=0)

y_train = df_train[y_var].values
y_test  = df_test[y_var].values
feature_names = X_train.columns.tolist()

# --- B√∫squeda aleatoria con GroupKFold por 'year' (en el train) ---
param_distributions = {
    "max_depth": [3, 5, 7, 10, None],
    "min_samples_leaf": randint(1, 31),      # 1..30
    "min_samples_split": randint(2, 21),     # 2..20
    "criterion": ["squared_error", "absolute_error"],  # MSE o MAE
}

cv = GroupKFold(n_splits=4)
groups = df_train["year"].astype(str).values  # agrupa por a√±o

base_tree = DecisionTreeRegressor(random_state=42)
search = RandomizedSearchCV(
    estimator=base_tree,
    param_distributions=param_distributions,
    n_iter=16,
    scoring="neg_mean_squared_error",  # compatible con versiones antiguas
    cv=cv,
    random_state=42,
    n_jobs=-1,
    verbose=0,
)

MODEL_DIR = Path(PATHS["out_models"])
MODEL_DIR.mkdir(parents=True, exist_ok=True)
BEST_MODEL_PATH = MODEL_DIR / "tree_reg_ebitda_time_tuned.pkl"

if RUN_MODELING:
    search.fit(X_train, y_train, groups=groups)
    best_model = search.best_estimator_

    # Guardar artefactos + metadatos de preprocesado
    payload = {
        "model": best_model,
        "feature_names": feature_names,
        "clip_info": clip_info,
        "cat_vars": cat_vars,
        "num_vars": num_vars,
        "y_var": y_var,
        "params": best_model.get_params(),
        "cv_best_rmse": float(np.sqrt(-search.best_score_)),
    }
    joblib.dump(payload, BEST_MODEL_PATH)
else:
    payload = joblib.load(BEST_MODEL_PATH)
    best_model   = payload["model"]
    feature_names = payload["feature_names"]
    clip_info     = payload["clip_info"]

# --- Evaluaci√≥n en test temporal (2016‚Äì2019) ---
y_pred = best_model.predict(X_test)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2   = r2_score(y_test, y_pred)

print("== Tuning del √°rbol (validaci√≥n sin fuga temporal) ==")
if RUN_MODELING:
    print("Mejores hiperpar√°metros:", payload["params"])
    print(f"CV best RMSE (train, GroupKFold por year): {payload['cv_best_rmse']:.4f}")
else:
    print("Modelo cargado de artefacto:", BEST_MODEL_PATH.name)

print(f"Test RMSE (2016‚Äì2019): {rmse:.4f}")
print(f"Test R¬≤   (2016‚Äì2019): {r2:.4f}")

# Importancia de variables (top 10)
importances = pd.Series(best_model.feature_importances_, index=feature_names).sort_values(ascending=False)
print("\nTop-10 importancias (mejor √°rbol):")
print(importances.head(10))

"""### Resultados finales tras el ajuste de hiperpar√°metros

Tras aplicar un proceso de **tuning de hiperpar√°metros** mediante b√∫squeda aleatoria y validaci√≥n cruzada **GroupKFold** agrupada por a√±o (evitando fuga temporal), hemos conseguido mejorar significativamente el rendimiento del modelo de √°rbol de regresi√≥n.  

Los mejores hiperpar√°metros encontrados fueron:  
- `max_depth`: 5  
- `min_samples_leaf`: 4  
- `min_samples_split`: 15  
- `criterion`: `"squared_error"`  
- Resto de par√°metros en valores por defecto, salvo `random_state=42` para reproducibilidad.  

Con esta configuraci√≥n, el modelo ha alcanzado:  
- **RMSE (CV best, train):** 0.9617  
- **RMSE (test 2016‚Äì2019):** 1.2257  
- **R¬≤ (test 2016‚Äì2019):** 0.6332  

Este incremento de R¬≤ respecto a la versi√≥n inicial (‚âà0,40) confirma que la optimizaci√≥n de hiperpar√°metros ha permitido mejorar la capacidad predictiva del √°rbol, manteniendo un modelo interpretable y sin sobreajustar a datos futuros.  

En cuanto a la **importancia de variables**, el patr√≥n final es m√°s equilibrado:  
1. **`sector_G`** (42,2%) como principal predictor.  
2. **`leverage`** (15,1%), **`country_FR`** (14,8%) y **`year`** (14,2%) como factores secundarios de peso similar.  
3. **`size`** (10,2%) gana relevancia respecto a modelos anteriores.  
4. **`solvency`** (2,2%) y **`total_assets`** (1,3%) mantienen una contribuci√≥n reducida.  

Con estos resultados, **este ser√° el modelo con el que trabajaremos en adelante**, ya que ofrece un compromiso s√≥lido entre rendimiento, realismo en la evaluaci√≥n y facilidad de interpretaci√≥n.  

En un escenario orientado exclusivamente a maximizar precisi√≥n, podr√≠amos explorar t√©cnicas de ensamble como **Random Forest** o **Gradient Boosting**, que probablemente elevar√≠an el R¬≤, especialmente con m√°s variables y ajustes finos.

## 6.Efecto causal de *Leverage* (alto vs. bajo) sobre `ebitda_net_income` con √°rboles

**Objetivo.** Estimar el **CATE** (*Conditional Average Treatment Effect*) del *apalancamiento financiero* (**Leverage**) sobre `ebitda_net_income`, **controlando** por el resto de variables observables: `total_assets`, `solvency` y las categ√≥ricas `country`, `sector`, `size`, `year`.

**Definici√≥n del tratamiento.** Creamos una variable binaria:
- `T = 1` (*Leverage alto*) si el `leverage` de una observaci√≥n **supera o iguala** el **percentil 50 por sector** (mediana **calculada sobre el conjunto de entrenamiento**).
- `T = 0` en caso contrario (*Leverage bajo*).

Esta regla por sector busca un umbral **equilibrado y comparable** entre actividades con niveles t√≠picos de apalancamiento distintos.

**Metodolog√≠a (T-learner con √°rboles).**
1. Partici√≥n temporal (como en el bloque predictivo): **train = 2000‚Äì2015**, **test = 2016‚Äì2019**.
2. Preprocesado **solo con informaci√≥n de train**:
   - *Clipping* p1‚Äìp99 en `total_assets`, `solvency`, `leverage` (mitigar outliers).
   - *One-hot* de categ√≥ricas con alineaci√≥n de columnas entre train/test.
3. Estimamos dos modelos de **√°rbol de regresi√≥n**:
   - `f1(x)`: entrenado **solo con tratados** (T=1) para aproximar el resultado potencial *con* tratamiento.
   - `f0(x)`: entrenado **solo con control** (T=0) para aproximar el resultado potencial *sin* tratamiento.
4. Para cada observaci√≥n del test, el **CATE** se estima como:  
  $$
\widehat{\mathrm{CATE}}(x)=\hat{y}_{1}(x)-\hat{y}_{0}(x)
$$
5. Reportamos:
   - **ATE (media del CATE)** y percentiles (p10, p50, p90) en test.
   - **CATE por subgrupos** (`country`, `sector`, `size`).
   - Histograma/boxplots de CATE.

**Reproducibilidad.** En `RUN_MODELING=True` entrenamos y guardamos:
- √Årboles `f1` y `f0`, umbrales por sector, *clip info* y columnas (*one-hot*).
- CSV con CATE a nivel de observaci√≥n y res√∫menes por subgrupos.
En `RUN_MODELING=False` **cargamos** esos artefactos para evitar recalcular.

**Supuestos clave (enunciado).** Interpretamos el efecto como causal bajo:
- **Ignorabilidad condicional**: dado el conjunto de controles, el tratamiento es ‚Äúcomo aleatorio‚Äù.
- **Solapamiento**: existen unidades comparables en T=1 y T=0.
- **SUTVA**: no hay interferencia entre unidades ni m√∫ltiples versiones del tratamiento.
"""

# 6) CATE de Leverage alto vs bajo con T-learner (√°rboles de decisi√≥n)

# --- Configuraci√≥n de rutas ---
PROC = Path(PATHS["data_proc"]) / "empresas_limpio.csv"
OUT_MODELS = Path(PATHS["out_models"]); OUT_MODELS.mkdir(parents=True, exist_ok=True)
OUT_CSV    = Path(PATHS["out_csv"]);    OUT_CSV.mkdir(parents=True, exist_ok=True)
OUT_FIGS   = Path(PATHS["out_figs"]);   OUT_FIGS.mkdir(parents=True, exist_ok=True)

ARTEFACTO = OUT_MODELS / "cate_tlearner_trees.pkl"
CATE_CSV  = OUT_CSV / "cate_test_rows.csv"
CATE_GRP  = OUT_CSV / "cate_test_groups.csv"
CATE_FIG  = OUT_FIGS / "cate_hist.png"

# --- Variables ---
y_var    = "ebitda_net_income"
cat_vars = ["country", "sector", "size", "year"]  # year lo tratamos como categ√≥rica (one-hot) por coherencia
num_vars = ["total_assets", "solvency", "leverage"]
treat_var = "leverage"   # continuo; se binariza a T usando mediana por sector (calculada en train)

# --- Lectura de datos completos ---
df_all = pd.read_csv(PROC).copy()

# --- Split temporal coherente con el bloque predictivo ---
train_mask = df_all["year"] <= 2015
df_train = df_all.loc[train_mask].copy()
df_test  = df_all.loc[~train_mask].copy()

# --- Definir tratamiento: mediana por sector calculada en TRAIN ---
sector_median = df_train.groupby("sector")[treat_var].median().to_dict()
global_median = df_train[treat_var].median()

def leverage_high(row):
    thr = sector_median.get(row["sector"], global_median)
    return 1 if row[treat_var] >= thr else 0

df_train["T"] = df_train.apply(leverage_high, axis=1)
df_test["T"]  = df_test.apply(leverage_high, axis=1)

# Info de balance
n_treat_tr, n_ctrl_tr = df_train["T"].sum(), (1 - df_train["T"]).sum()
print(f"Train balance ‚Üí T=1: {n_treat_tr} | T=0: {n_ctrl_tr}")

# --- Preprocesado SOLO con info de TRAIN: clipping p1‚Äìp99 en num√©ricas ---
clip_info = {}
for col in num_vars:
    q1 = float(df_train[col].quantile(0.01)); q99 = float(df_train[col].quantile(0.99))
    clip_info[col] = (q1, q99)
    df_train[col] = df_train[col].clip(q1, q99)
    df_test[col]  = df_test[col].clip(q1, q99)

# --- One-hot encoding & alineaci√≥n de columnas (train como referencia) ---
X_train_full = pd.get_dummies(df_train[cat_vars + num_vars], drop_first=True)
X_test_full  = pd.get_dummies(df_test[cat_vars + num_vars],  drop_first=True)
X_test_full  = X_test_full.reindex(columns=X_train_full.columns, fill_value=0)

y_train = df_train[y_var].values
y_test  = df_test[y_var].values

# --- Subconjuntos para T-learner (solo train) ---
X1 = X_train_full[df_train["T"] == 1]
y1 = df_train.loc[df_train["T"] == 1, y_var].values

X0 = X_train_full[df_train["T"] == 0]
y0 = df_train.loc[df_train["T"] == 0, y_var].values

if RUN_MODELING:
    # Hiperpar√°metros sobrios (interpretables) ‚Äî puedes ajustar si lo ves necesario
    base_params = dict(max_depth=5, min_samples_leaf=10, min_samples_split=20,
                       criterion="squared_error", random_state=42)

    f1 = DecisionTreeRegressor(**base_params).fit(X1, y1)  # resultado potencial con tratamiento
    f0 = DecisionTreeRegressor(**base_params).fit(X0, y0)  # resultado potencial sin tratamiento

    payload = {
        "f1": f1, "f0": f0,
        "feature_names": X_train_full.columns.tolist(),
        "sector_median": sector_median, "global_median": global_median,
        "clip_info": clip_info, "cat_vars": cat_vars, "num_vars": num_vars,
        "y_var": y_var, "treat_var": treat_var
    }
    joblib.dump(payload, ARTEFACTO)
else:
    payload = joblib.load(ARTEFACTO)
    f1, f0 = payload["f1"], payload["f0"]

# --- C√°lculo de CATE en TEST ---
# (Aplicamos los modelos entrenados en train a TODO el test)
y1_hat = f1.predict(X_test_full)
y0_hat = f0.predict(X_test_full)
cate   = y1_hat - y0_hat  # CATE(x) estimado

# --- Resumen global ---
ate = float(np.mean(cate))
p10, p50, p90 = np.percentile(cate, [10, 50, 90])

print("\n=== CATE en test (2016‚Äì2019). Interpretaci√≥n: CATE = efecto de Leverage ALTO ===")
print(f"ATE (media CATE): {ate:.4f}")
print(f"Percentiles CATE (p10, p50, p90): {p10:.4f}, {p50:.4f}, {p90:.4f}")

# --- CATE por subgrupos (country, sector, size) ---
df_out = df_test[["country", "sector", "size", "year", "leverage", y_var, "T"]].copy()
df_out["y1_hat"] = y1_hat
df_out["y0_hat"] = y0_hat
df_out["cate"]   = cate

grp_country = df_out.groupby("country")["cate"].agg(["count","mean","median","std"]).reset_index()
grp_sector  = df_out.groupby("sector")["cate"].agg(["count","mean","median","std"]).reset_index()
grp_size    = df_out.groupby("size")["cate"].agg(["count","mean","median","std"]).reset_index()

grp_country["group"] = "country"; grp_sector["group"] = "sector"; grp_size["group"] = "size"
grp_country.rename(columns={"country":"level"}, inplace=True)
grp_sector.rename(columns={"sector":"level"}, inplace=True)
grp_size.rename(columns={"size":"level"}, inplace=True)
grp_all = pd.concat([grp_country, grp_sector, grp_size], ignore_index=True)

# --- Guardar salidas ---
df_out.to_csv(CATE_CSV, index=False, encoding="utf-8")
grp_all.to_csv(CATE_GRP, index=False, encoding="utf-8")
print(f"\nCSV guardados:\n- {CATE_CSV}\n- {CATE_GRP}")

# --- Visualizaci√≥n r√°pida del CATE ---
plt.figure(figsize=(6,4))
sns.histplot(df_out["cate"], bins=30, kde=True)
plt.title("Distribuci√≥n del CATE estimado (test 2016‚Äì2019)")
plt.xlabel("CATE (y1_hat - y0_hat)"); plt.ylabel("Frecuencia")
plt.tight_layout()
plt.savefig(CATE_FIG, dpi=120)
plt.show()
print(f"Figura guardada en: {CATE_FIG}")

"""### Interpretaci√≥n global del efecto causal del leverage

En esta fase hemos estimado el efecto causal del apalancamiento financiero (**Leverage alto vs. Leverage bajo**) sobre el ratio **EBITDA/Ingresos netos**, controlando por el resto de variables observables.  

Los resultados obtenidos para el periodo de test (2016‚Äì2019) son los siguientes:

- **ATE (Average Treatment Effect):** 0.1857  
  ‚Üí En promedio, las empresas con *Leverage alto* presentan un ratio EBITDA/Ingresos netos ligeramente superior (‚âà0,19 unidades) respecto a las de *Leverage bajo*.  

- **Percentiles del CATE:**  
  - p10 = -1.79 ‚Üí en el 10% de las observaciones, el apalancamiento alto reduce notablemente la rentabilidad relativa.  
  - p50 = -0.30 ‚Üí la mediana muestra un efecto levemente negativo, lo que indica que en la mayor√≠a de casos el leverage alto no resulta beneficioso.  
  - p90 = 3.27 ‚Üí en el 10% superior, el efecto es muy positivo, reflejando que existen subgrupos de empresas donde el leverage alto impulsa de manera significativa la rentabilidad operativa.  

Estos resultados ponen de manifiesto la **heterogeneidad del impacto del leverage**: aunque el efecto medio es positivo, existen diferencias importantes entre empresas. El histograma confirma esta dispersi√≥n, mostrando tanto efectos negativos como positivos en distintos casos.

En el siguiente punto analizaremos los **efectos por subgrupos (pa√≠s, sector, tama√±o)** utilizando los resultados almacenados en `cate_test_groups.csv`, con el fin de identificar en qu√© contextos el apalancamiento resulta m√°s o menos favorable.

## 7.Resultados por subgrupos (pa√≠s, sector, tama√±o)
"""

# === Punto 7. Resultados por subgrupos (pa√≠s, sector, tama√±o) ===

# Cargar resultados guardados de CATE
df_groups = pd.read_csv(Path(PATHS["out_csv"]) / "cate_test_groups.csv")
df_rows   = pd.read_csv(Path(PATHS["out_csv"]) / "cate_test_rows.csv")

print("=== Resultados por subgrupo ===")
display(df_groups.head(10))

print("\n=== Primeras filas con CATE individuales ===")
display(df_rows.head(10))

# Gr√°fico de barras: efecto medio por subgrupo
plt.figure(figsize=(10,6))
sns.barplot(
    data=df_groups,
    x="mean",
    y="level",
    hue="group",
    dodge=False
)
plt.axvline(0, color="red", linestyle="--", linewidth=1)
plt.title("Efecto causal estimado (CATE medio) por subgrupo")
plt.xlabel("CATE medio (Leverage alto - bajo)")
plt.ylabel("Subgrupo")
plt.tight_layout()
plt.show()

# Crear tabla resumen ordenada por CATE medio ordenado de mayor efecto positivo al m√°s negativo
df_summary = (
    df_groups[["group", "level", "mean", "median", "std", "count"]]
    .sort_values(by="mean", ascending=False)
    .round(3)
    .reset_index(drop=True)
)

print("=== Resumen de CATE medio por subgrupo ===")
display(df_summary)

"""### Resultados del efecto causal por subgrupos (CATE)

Tras estimar el **Conditional Average Treatment Effect (CATE)** del apalancamiento financiero alto vs. bajo sobre el ratio *EBITDA/Ingresos netos*, obtenemos diferencias relevantes seg√∫n pa√≠s, sector y tama√±o de las empresas.  

Los resultados muestran lo siguiente:

- **Por pa√≠s**:  
  - En **Espa√±a (ES)** el efecto medio es **positivo y elevado** (+1,05), indicando que un mayor apalancamiento se asocia con un incremento del rendimiento operativo.  
  - En **Francia (FR)** el efecto medio es **negativo** (-0,68), lo que sugiere que un mayor endeudamiento reduce la rentabilidad en este contexto.  

- **Por sector**:  
  - En el **sector C** el efecto es claramente **positivo** (+0,92), con mediana incluso superior a 1,3.  
  - En el **sector F** el efecto es pr√°cticamente **nulo** (media ‚âà 0,0), aunque con alta dispersi√≥n (desviaci√≥n > 2,1).  
  - En el **sector G** el efecto medio es **ligeramente negativo** (-0,33).  

- **Por tama√±o**:  
  - Las **empresas peque√±as (size=1)** muestran un **efecto positivo notable** (+0,95), lo que sugiere que el apalancamiento puede ayudar a impulsar su EBITDA/Ingresos netos.  
  - Las de tama√±o **medio (size=2)** y **grande (size=3)** presentan efectos **negativos** (-0,19 y -0,20 respectivamente), lo que apunta a que en organizaciones m√°s grandes un mayor endeudamiento no se traduce en mejor rendimiento operativo.  

En conjunto, se observa que el impacto del apalancamiento es **heterog√©neo**:  
- Beneficioso en Espa√±a, en el sector C y en empresas peque√±as.  
- Negativo en Francia y en compa√±√≠as de mayor tama√±o.  
- Neutro o incierto en sectores como F y G.  

Esto resalta la importancia de considerar el **contexto empresarial y sectorial** antes de recomendar pol√≠ticas de financiaci√≥n basadas en mayor deuda.

## 8.Conclusiones generales del ejercicio

En este ejercicio hemos desarrollado un an√°lisis completo para estudiar el impacto de las caracter√≠sticas estructurales de las empresas sobre el ratio **EBITDA/Ingresos netos** y, en particular, para evaluar el efecto causal del apalancamiento financiero.  

El proceso ha seguido varias etapas:  
1. **Establecimiento de un baseline** con un modelo de regresi√≥n lineal, que sirvi√≥ como referencia inicial y permiti√≥ constatar sus limitaciones para capturar la complejidad del fen√≥meno.  
2. **Construcci√≥n de un modelo predictivo** mediante un √°rbol de regresi√≥n, donde se probaron distintas configuraciones (estratificaci√≥n por a√±o, transformaci√≥n logar√≠tmica de activos totales). Tras comparar alternativas, se opt√≥ por un enfoque con **partici√≥n temporal estricta** (train: 2000‚Äì2015, test: 2016‚Äì2019) y posterior **ajuste de hiperpar√°metros**, lo que permiti√≥ mejorar las m√©tricas hasta alcanzar un **R¬≤ ‚âà 0.63** en test.  
3. **Interpretaci√≥n de la importancia de las variables**, identificando a `sector`, `country`, `year` y `leverage` como los principales determinantes del rendimiento operativo en el modelo ajustado.  
4. **Estimaci√≥n del efecto causal (CATE)** del apalancamiento alto frente a bajo, controlando por el resto de variables. Para ello se aplicaron modelos basados en √°rboles, obteniendo tanto una medida global (ATE ‚âà 0.19) como resultados espec√≠ficos por subgrupos.  
5. **An√°lisis de heterogeneidad por pa√≠s, sector y tama√±o**, que permiti√≥ detectar diferencias sustanciales en c√≥mo impacta el endeudamiento en cada contexto.  

En s√≠ntesis, los resultados muestran que el apalancamiento financiero no tiene un efecto uniforme sobre la rentabilidad: **su impacto depende del contexto empresarial y sectorial**. Esto implica que las decisiones de financiaci√≥n deben adaptarse a las caracter√≠sticas de cada compa√±√≠a y no plantearse como una estrategia homog√©nea para todos los casos.
"""