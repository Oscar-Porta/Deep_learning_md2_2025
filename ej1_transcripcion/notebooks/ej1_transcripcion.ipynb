{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1z_w1KivVCV4R_QeMvAtoaN7sZVabO3Gl","timestamp":1754743815471}],"gpuType":"T4","authorship_tag":"ABX9TyNC0KmzWkD0yr6bay9gUISW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Ejercicio 1 — Transcripción de audios y extracción de entidades (NER)\n","\n","En este ejercicio vamos a resolver el problema en dos etapas: primero convertimos los audios en español a texto mediante **Whisper** y, después, aplicamos **NER** con el modelo preentrenado `MMG/xlm-roberta-large-ner-spanish` para detectar entidades como personas (PER), organizaciones (ORG), ubicaciones (LOC), fechas (DATE), etc. La salida solicitada por el enunciado será un **JSON por audio** y un **JSON global** con el identificador del archivo, la transcripción y las entidades agrupadas por tipo.\n","\n","**Pasos a seguir**\n","1. Preparación del entorno (Colab + Drive + rutas)\n","2. Verificación del entorno (Python y GPU)\n","3. Instalación de dependencias (Whisper y Transformers)  \n","4. Prueba piloto con un audio  \n","5. Procesamiento por lotes y guardado de resultados (JSON)  \n","6. Validación rápida y resumen  "],"metadata":{"id":"DBRrfIno6tBX"}},{"cell_type":"markdown","source":["## 1.Rutas, estructura y README\n","\n","Trabajamos habitualmente en **Google Drive** para guardar y reanudar fácilmente el ejercicio (`USE_DRIVE=True`).  \n","Para la **entrega**, usamos **rutas relativas** con `USE_DRIVE=False`, de modo que el cuaderno sea **portable** y se pueda **re-ejecutar** sin depender de Drive. El cuaderno es **reproducible**: al volver a ejecutarlo, transcribe de nuevo y regenera los JSON/CSV.\n","\n","Estructura creada:\n","- `notebooks/`, `src/`, `scripts/` (aquí dejaremos el **.py** exportado)\n","- `data/audio/` (aquí se copian los **audios**; no van en el ZIP)\n","- `outputs/json/`, `outputs/csv/`, `outputs/figures/`\n","- `outputs/docs/` (aquí guardaremos el **PDF** del informe)\n","- `outputs/logs/`\n","\n","El README indica dónde colocar **datos**, **.py** y **PDF**, y qué salidas se generan."],"metadata":{"id":"JVTwidoM7fq6"}},{"cell_type":"code","source":["#Rutas + estructura + README\n","from pathlib import Path\n","\n","USE_DRIVE = False  #True (monta Drive)\n","                   #False (rutas relativas, sin Drive)\n","\n","if USE_DRIVE:\n","    try:\n","        from google.colab import drive  # solo si estamos en Colab\n","        drive.mount('/content/drive')\n","    except Exception as e:\n","        print(\"Aviso: no se pudo montar Drive (¿no estás en Colab?).\", e)\n","    ROOT = Path(\"/content/drive/MyDrive/MASTER BIG DATA/md2_2025\")\n","    EJ1  = ROOT / \"ej1_transcripcion\"\n","else:\n","    here = Path(\".\").resolve()\n","    EJ1  = here if (here.name == \"ej1_transcripcion\") else (here / \"ej1_transcripcion\")\n","\n","PATHS = {\n","    \"root\":        str(EJ1.parent),\n","    \"ej1\":         str(EJ1),\n","    \"nb\":          str(EJ1 / \"notebooks\"),\n","    \"src\":         str(EJ1 / \"src\"),\n","    \"scripts\":     str(EJ1 / \"scripts\"),                 # .py exportado\n","    \"data_audio\":  str(EJ1 / \"data\" / \"audio\"),\n","    \"out_json\":    str(EJ1 / \"outputs\" / \"json\"),\n","    \"out_csv\":     str(EJ1 / \"outputs\" / \"csv\"),\n","    \"out_figs\":    str(EJ1 / \"outputs\" / \"figures\"),\n","    \"out_docs\":    str(EJ1 / \"outputs\" / \"docs\"),        # PDF del informe\n","    \"out_logs\":    str(EJ1 / \"outputs\" / \"logs\"),\n","}\n","\n","def ensure_dir(p: str): Path(p).mkdir(parents=True, exist_ok=True)\n","for k in (\"nb\",\"src\",\"scripts\",\"data_audio\",\"out_json\",\"out_csv\",\"out_figs\",\"out_docs\",\"out_logs\"):\n","    ensure_dir(PATHS[k])\n","\n","# README\n","readme = Path(PATHS[\"ej1\"]) / \"README.md\"\n","if not readme.exists():\n","    readme.write_text(\"\"\"# Ejercicio 1 — Transcripción + NER (ES)\n","\n","**Objetivo.** Transcribimos audios en español con Whisper y extraemos entidades (PER/ORG/LOC/MISC) con `MMG/xlm-roberta-large-ner-spanish`.\n","**Notebook principal:** `notebooks/ej1_transcripcion.ipynb`\n","\n","## Datos (no incluidos en el ZIP)\n","Copiar los audios en: `data/audio/` (formatos: wav, mp3, m4a, flac, ogg).\n","\n","## Salidas\n","- `outputs/json/`  → JSON por audio + `resultados_global.json`\n","- `outputs/csv/`   → resúmenes tabulares (si se generan)\n","- `outputs/figures/` → figuras opcionales (si se generan)\n","\n","## Entrega\n","- Exportar el notebook a **PDF** y guardarlo en `outputs/docs/`.\n","- Exportar a **.py** y colocarlo en `scripts/`.\n","\n","## Requisitos (Colab o local)\n","- `ffmpeg`, `openai-whisper`, `transformers`, `torch` (y aceleración si hay GPU).\n","\"\"\", encoding=\"utf-8\")\n","    print(\"README creado:\", readme)\n","else:\n","    print(\"README ya existe: no se modifica.\")\n","\n","print(\"Rutas clave:\")\n","for k in (\"data_audio\",\"out_json\",\"out_csv\",\"out_figs\",\"out_docs\",\"scripts\"):\n","    print(f\"  {k:>11} → {PATHS[k]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nDIrv6KA7iLd","executionInfo":{"status":"ok","timestamp":1755504278466,"user_tz":-120,"elapsed":17,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"38f418e8-4431-44dd-a39f-95b1d989fbe0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["README creado: /content/ej1_transcripcion/README.md\n","Rutas clave:\n","   data_audio → /content/ej1_transcripcion/data/audio\n","     out_json → /content/ej1_transcripcion/outputs/json\n","      out_csv → /content/ej1_transcripcion/outputs/csv\n","     out_figs → /content/ej1_transcripcion/outputs/figures\n","     out_docs → /content/ej1_transcripcion/outputs/docs\n","      scripts → /content/ej1_transcripcion/scripts\n"]}]},{"cell_type":"markdown","source":["## 2.Verificación del entorno (Python y GPU)\n","\n","Comprobamos la versión de Python y si disponemos de GPU. Esta información nos ayudará a decidir si ejecutamos los modelos en CPU o en GPU en los siguientes pasos."],"metadata":{"id":"bYtv9KsO94xx"}},{"cell_type":"code","source":["# Inspección rápida del entorno\n","\n","import platform\n","import torch\n","\n","print(\"Python:\", platform.python_version())\n","has_cuda = torch.cuda.is_available()\n","print(\"Torch CUDA disponible:\", has_cuda)\n","if has_cuda:\n","    print(\"GPU:\", torch.cuda.get_device_name(0))\n","else:\n","    print(\"Sin GPU.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NjBHfMaK91hS","executionInfo":{"status":"ok","timestamp":1755429136818,"user_tz":-120,"elapsed":26,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"b6950f61-fc42-437c-b24c-12bb7204b903"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python: 3.11.13\n","Torch CUDA disponible: True\n","GPU: Tesla T4\n"]}]},{"cell_type":"markdown","source":["## 3.Instalación de dependencias\n","\n","Instalamos las librerías necesarias para el ejercicio: **Whisper** (transcripción), **Transformers** (NER) y **ffmpeg** (necesario para procesar audio). Mantendremos las versiones provistas por Colab para maximizar la compatibilidad."],"metadata":{"id":"_exvTQWR-8hh"}},{"cell_type":"code","source":["# Instalación de librerías necesarias\n","!pip -q install -U openai-whisper transformers accelerate datasets\n","!apt -q install -y ffmpeg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J64jgOSl9959","executionInfo":{"status":"ok","timestamp":1755429152028,"user_tz":-120,"elapsed":15223,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"70314b7f-fa9b-4167-b65f-d9db4cf425d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hReading package lists...\n","Building dependency tree...\n","Reading state information...\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"]}]},{"cell_type":"code","source":["### Comprobación de la instalación\n","import sys\n","import transformers, whisper\n","\n","print(\"Python      :\", platform.python_version())\n","print(\"Torch       :\", torch.__version__)\n","print(\"Transformers:\", transformers.__version__)\n","# confirmamos import y ruta de instalación\n","print(\"Whisper     : OK (\", whisper.__file__, \")\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLK9ZSPBABWj","executionInfo":{"status":"ok","timestamp":1755429152082,"user_tz":-120,"elapsed":53,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"95d49cc8-8d45-4981-9b0a-d500b1541479"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python      : 3.11.13\n","Torch       : 2.6.0+cu124\n","Transformers: 4.55.1\n","Whisper     : OK ( /usr/local/lib/python3.11/dist-packages/whisper/__init__.py )\n"]}]},{"cell_type":"code","source":["#Importación resto de librerias\n","# Estándar\n","import glob, json, csv\n","from datetime import datetime\n","from collections import Counter, defaultdict\n","\n","# Terceros\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n"],"metadata":{"id":"FcRfhIUCSMyh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.Carga de audios de prueba\n","\n","Listamos los audios disponibles en `ej1_transcripcion/data/audio/` y seleccionamos automáticamente el primero como muestra de trabajo. Este paso nos permite verificar que la carpeta de datos está correctamente configurada antes de continuar."],"metadata":{"id":"p_hkX-HGAKfd"}},{"cell_type":"code","source":["AUDIO_DIR = PATHS[\"data_audio\"]\n","\n","# Listamos extensiones comunes de audio por si en un futuro añadimos mas archivos\n","exts = (\"*.wav\", \"*.mp3\", \"*.m4a\", \"*.flac\", \"*.ogg\")\n","audio_paths = []\n","for ext in exts:\n","    audio_paths.extend(glob.glob(os.path.join(AUDIO_DIR, ext)))\n","\n","audio_paths = sorted(audio_paths)\n","\n","print(\"Carpeta de audio:\", AUDIO_DIR)\n","print(\"Nº de archivos encontrados:\", len(audio_paths))\n","for p in audio_paths[:10]:\n","    print(\" -\", os.path.basename(p))\n","\n","# Seleccionamos un audio de ejemplo\n","AUDIO_SAMPLE = audio_paths[0] if audio_paths else None\n","print(\"Audio de ejemplo seleccionado:\", AUDIO_SAMPLE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_wrwyFfEeJq","executionInfo":{"status":"ok","timestamp":1755429152095,"user_tz":-120,"elapsed":5,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"efaab997-04f1-4872-8bde-09f193c981b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Carpeta de audio: /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/data/audio\n","Nº de archivos encontrados: 80\n"," - frase_01.wav\n"," - frase_02.wav\n"," - frase_03.wav\n"," - frase_04.wav\n"," - frase_05.wav\n"," - frase_06.wav\n"," - frase_07.wav\n"," - frase_08.wav\n"," - frase_09.wav\n"," - frase_10.wav\n","Audio de ejemplo seleccionado: /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/data/audio/frase_01.wav\n"]}]},{"cell_type":"markdown","source":["### 4.1Prueba piloto con un audio\n","\n","En esta sección cargamos los modelos necesarios, transcribimos el audio de ejemplo y ejecutamos el reconocimiento de entidades (NER). Después agrupamos las entidades por tipo y construimos un diccionario con el formato de salida que utilizaremos más adelante (JSON), manteniéndolo de momento en memoria para validar el flujo.\n","\n","**Pasos:**\n","1. Cargar Whisper (tamaño `base`) y el modelo NER `MMG/xlm-roberta-large-ner-spanish`.\n","2. Transcribir el archivo de audio seleccionado automáticamente.\n","3. Ejecutar NER sobre la transcripción.\n","4. Agrupar las entidades por tipo y mostrar un resumen (conteos por tipo y primeros ejemplos)."],"metadata":{"id":"e7BdtQMIGdDI"}},{"cell_type":"code","source":["# 1) Dispositivo\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# 2) Modelos\n","WHISPER_SIZE = \"base\"  # se podrá cambiar a 'small'/'medium' más adelante si lo necesitamos\n","NER_MODEL = \"MMG/xlm-roberta-large-ner-spanish\"\n","\n","whisper_model = whisper.load_model(WHISPER_SIZE, device=device)\n","\n","tokenizer = AutoTokenizer.from_pretrained(NER_MODEL)\n","ner_model = AutoModelForTokenClassification.from_pretrained(NER_MODEL)\n","ner_pipe = pipeline(\n","    \"token-classification\",\n","    model=ner_model,\n","    tokenizer=tokenizer,\n","    aggregation_strategy=\"simple\",\n","    device=0 if device == \"cuda\" else -1\n",")\n","\n","# 3) Audio de ejemplo (seleccionado en la sección anterior)\n","audio_path = AUDIO_SAMPLE\n","fname = os.path.basename(audio_path) if audio_path else None\n","\n","# 4) Transcripción\n","transcripcion = \"\"\n","if audio_path:\n","    result = whisper_model.transcribe(audio_path, language=\"es\", verbose=False)\n","    transcripcion = (result.get(\"text\") or \"\").strip()\n","\n","# 5) NER\n","entidades_por_tipo = {}\n","if transcripcion:\n","    ents = ner_pipe(transcripcion)\n","    for ent in ents:\n","        etype = ent.get(\"entity_group\", \"MISC\")\n","        item = {\n","            \"texto\": ent.get(\"word\", \"\"),\n","            \"start\": int(ent.get(\"start\", 0)),\n","            \"end\": int(ent.get(\"end\", 0)),\n","            \"score\": float(ent.get(\"score\", 0.0)),\n","        }\n","        entidades_por_tipo.setdefault(etype, []).append(item)\n","\n","# 6) Resultado piloto en memoria (formato final que usaremos para JSON)\n","resultado_piloto = {\n","    \"archivo_audio\": fname,\n","    \"ruta_audio\": audio_path,\n","    \"transcripcion\": transcripcion,\n","    \"entidades\": entidades_por_tipo,\n","    \"modelo_whisper\": WHISPER_SIZE,\n","    \"modelo_ner\": NER_MODEL,\n","    \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n","}\n","\n","# 7) Resumen legible\n","print(\"Archivo:\", resultado_piloto[\"archivo_audio\"])\n","print(\"\\nTranscripción (primeros 200 caracteres):\")\n","print(resultado_piloto[\"transcripcion\"][:200])\n","\n","print(\"\\nEntidades detectadas por tipo:\")\n","for t, lista in resultado_piloto[\"entidades\"].items():\n","    print(f\" - {t}: {len(lista)}\")\n","\n","# 8) Muestra de las primeras 3 entidades de cada tipo (si existen)\n","for t, lista in resultado_piloto[\"entidades\"].items():\n","    if not lista:\n","        continue\n","    print(f\"\\nEjemplos [{t}]:\")\n","    for e in lista[:3]:\n","        print(f\"  • {e['texto']} (score={e['score']:.3f})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jtsaXAWkGj5X","executionInfo":{"status":"ok","timestamp":1755429172379,"user_tz":-120,"elapsed":20283,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"115e0ccb-206b-4c03-cd37-3a87588f6019"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n","100%|██████████| 405/405 [00:02<00:00, 181.94frames/s]\n"]},{"output_type":"stream","name":"stdout","text":["Archivo: frase_01.wav\n","\n","Transcripción (primeros 200 caracteres):\n","Sofia viajó a Sevilla para asistir a un Congreso de Medicina.\n","\n","Entidades detectadas por tipo:\n"," - PER: 1\n"," - LOC: 1\n"," - MISC: 1\n","\n","Ejemplos [PER]:\n","  • Sofia (score=0.983)\n","\n","Ejemplos [LOC]:\n","  • Sevilla (score=0.999)\n","\n","Ejemplos [MISC]:\n","  • Congreso de Medicina (score=0.979)\n"]}]},{"cell_type":"markdown","source":["## 5.Procesamiento por lotes y guardado de resultados (JSON)\n","\n","Aplicamos el flujo validado a todos los audios de `ej1_transcripcion/data/audio/`. Para cada archivo generamos un **JSON individual** con: `archivo_audio`, `ruta_audio`, `transcripcion`, `entidades` (agrupadas por tipo), `modelo_whisper`, `modelo_ner` y `timestamp`. Además, consolidamos todo en un **JSON global**. Los guardados se realizan en `ej1_transcripcion/outputs/json/`."],"metadata":{"id":"D5AOAwaJJ6GH"}},{"cell_type":"code","source":["# --- Utilidades ---\n","def transcribir_audio(path_audio, model, language=\"es\"):\n","    \"\"\"Transcribe un audio con Whisper y devuelve el texto (str).\"\"\"\n","    out = model.transcribe(path_audio, language=language, verbose=False)\n","    return (out.get(\"text\") or \"\").strip()\n","\n","def extraer_entidades(texto, ner_pipeline):\n","    \"\"\"Ejecuta NER y agrupa por tipo. Devuelve dict {tipo: [..]}.\"\"\"\n","    if not texto:\n","        return {}\n","    raw = ner_pipeline(texto)\n","    entidades = {}\n","    for ent in raw:\n","        etype = ent.get(\"entity_group\", \"MISC\")\n","        item = {\n","            \"texto\": ent.get(\"word\", \"\"),\n","            \"start\": int(ent.get(\"start\", 0)),\n","            \"end\": int(ent.get(\"end\", 0)),\n","            \"score\": float(ent.get(\"score\", 0.0)),\n","        }\n","        entidades.setdefault(etype, []).append(item)\n","    return entidades\n","\n","# --- Garantizar modelos en memoria (por si la sesión se reinició) ---\n","try:\n","    whisper_model\n","    ner_pipe\n","    WHISPER_SIZE\n","    NER_MODEL\n","except NameError:\n","    import torch, whisper\n","    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    WHISPER_SIZE = \"base\"\n","    NER_MODEL = \"MMG/xlm-roberta-large-ner-spanish\"\n","    whisper_model = whisper.load_model(WHISPER_SIZE, device=device)\n","    tok = AutoTokenizer.from_pretrained(NER_MODEL)\n","    ner_model = AutoModelForTokenClassification.from_pretrained(NER_MODEL)\n","    ner_pipe = pipeline(\n","        \"token-classification\",\n","        model=ner_model,\n","        tokenizer=tok,\n","        aggregation_strategy=\"simple\",\n","        device=0 if device == \"cuda\" else -1\n","    )\n","\n","# --- Localizar audios y preparar salida ---\n","AUDIO_DIR = PATHS[\"data_audio\"]\n","OUT_JSON_DIR = PATHS[\"out_json\"]\n","os.makedirs(OUT_JSON_DIR, exist_ok=True)\n","\n","exts = (\"*.wav\", \"*.mp3\", \"*.m4a\", \"*.flac\", \"*.ogg\")\n","audio_paths = []\n","for ext in exts:\n","    audio_paths.extend(glob.glob(os.path.join(AUDIO_DIR, ext)))\n","audio_paths = sorted(audio_paths)\n","\n","resultados = []\n","contador_tipos = Counter()\n","procesados = 0\n","fallidos = 0\n","\n","# --- Bucle principal ---\n","for apath in audio_paths:\n","    fname = os.path.basename(apath)\n","    try:\n","        # 1) Transcripción\n","        texto = transcribir_audio(apath, whisper_model, language=\"es\")\n","\n","        # 2) NER\n","        entidades = extraer_entidades(texto, ner_pipe)\n","\n","        # 3) Registro y guardado JSON individual\n","        registro = {\n","            \"archivo_audio\": fname,\n","            \"ruta_audio\": apath,\n","            \"transcripcion\": texto,\n","            \"entidades\": entidades,\n","            \"modelo_whisper\": WHISPER_SIZE,\n","            \"modelo_ner\": NER_MODEL,\n","            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n","        }\n","        resultados.append(registro)\n","\n","        # Acumulados\n","        for t, lst in entidades.items():\n","            contador_tipos[t] += len(lst)\n","\n","        # JSON por audio\n","        out_path = os.path.join(OUT_JSON_DIR, f\"{fname}.json\")\n","        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(registro, f, ensure_ascii=False, indent=2)\n","\n","        procesados += 1\n","\n","    except Exception as e:\n","        fallidos += 1\n","        # Guardamos un JSON de error para trazabilidad\n","        registro_err = {\n","            \"archivo_audio\": fname,\n","            \"ruta_audio\": apath,\n","            \"error\": str(e),\n","            \"modelo_whisper\": WHISPER_SIZE,\n","            \"modelo_ner\": NER_MODEL,\n","            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n","        }\n","        resultados.append(registro_err)\n","        out_path = os.path.join(OUT_JSON_DIR, f\"{fname}.error.json\")\n","        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(registro_err, f, ensure_ascii=False, indent=2)\n","\n","# --- JSON global ---\n","GLOBAL_JSON = os.path.join(OUT_JSON_DIR, \"resultados_global.json\")\n","with open(GLOBAL_JSON, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(resultados, f, ensure_ascii=False, indent=2)\n","\n","# --- Resumen en consola ---\n","print(\"Audios totales     :\", len(audio_paths))\n","print(\"Procesados OK      :\", procesados)\n","print(\"Con error          :\", fallidos)\n","print(\"Salida (JSON por audio):\", OUT_JSON_DIR)\n","print(\"JSON global        :\", GLOBAL_JSON)\n","print(\"\\nEntidades por tipo (acumuladas):\")\n","for t, c in sorted(contador_tipos.items(), key=lambda x: (-x[1], x[0])):\n","    print(f\" - {t}: {c}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BpsqIJ_VJ_9K","executionInfo":{"status":"ok","timestamp":1755429347259,"user_tz":-120,"elapsed":174863,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"4ad30f5a-ac16-45db-b2b3-c7d55e56bb1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|██████████| 405/405 [00:00<00:00, 1209.52frames/s]\n","100%|██████████| 350/350 [00:00<00:00, 2074.54frames/s]\n","100%|██████████| 358/358 [00:00<00:00, 857.10frames/s]\n","100%|██████████| 417/417 [00:00<00:00, 570.42frames/s]\n","100%|██████████| 387/387 [00:00<00:00, 462.65frames/s]\n","100%|██████████| 328/328 [00:00<00:00, 835.20frames/s]\n","100%|██████████| 368/368 [00:00<00:00, 808.63frames/s]\n","100%|██████████| 369/369 [00:00<00:00, 1002.32frames/s]\n","100%|██████████| 383/383 [00:00<00:00, 987.69frames/s] \n","100%|██████████| 339/339 [00:00<00:00, 526.17frames/s]\n","You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","100%|██████████| 460/460 [00:00<00:00, 517.99frames/s]\n","100%|██████████| 387/387 [00:00<00:00, 2153.78frames/s]\n","100%|██████████| 443/443 [00:00<00:00, 1770.22frames/s]\n","100%|██████████| 367/367 [00:00<00:00, 1647.97frames/s]\n","100%|██████████| 413/413 [00:00<00:00, 1793.31frames/s]\n","100%|██████████| 377/377 [00:00<00:00, 1799.79frames/s]\n","100%|██████████| 332/332 [00:00<00:00, 1849.88frames/s]\n","100%|██████████| 478/478 [00:00<00:00, 1282.39frames/s]\n","100%|██████████| 310/310 [00:04<00:00, 64.39frames/s]\n","100%|██████████| 340/340 [00:01<00:00, 289.01frames/s]\n","100%|██████████| 329/329 [00:00<00:00, 599.86frames/s]\n","100%|██████████| 324/324 [00:00<00:00, 1565.33frames/s]\n","100%|██████████| 375/375 [00:00<00:00, 1762.12frames/s]\n","100%|██████████| 467/467 [00:00<00:00, 2009.04frames/s]\n","100%|██████████| 407/407 [00:00<00:00, 1938.56frames/s]\n","100%|██████████| 363/363 [00:00<00:00, 1879.74frames/s]\n","100%|██████████| 367/367 [00:00<00:00, 1966.52frames/s]\n","100%|██████████| 310/310 [00:00<00:00, 1526.75frames/s]\n","100%|██████████| 349/349 [00:00<00:00, 1231.33frames/s]\n","100%|██████████| 361/361 [00:00<00:00, 2292.13frames/s]\n","100%|██████████| 396/396 [00:00<00:00, 1897.78frames/s]\n","100%|██████████| 299/299 [00:00<00:00, 1758.72frames/s]\n","100%|██████████| 394/394 [00:00<00:00, 1450.89frames/s]\n","100%|██████████| 418/418 [00:00<00:00, 1888.58frames/s]\n","100%|██████████| 438/438 [00:00<00:00, 658.46frames/s]\n","100%|██████████| 407/407 [00:01<00:00, 310.25frames/s]\n","100%|██████████| 436/436 [00:00<00:00, 448.92frames/s]\n","100%|██████████| 397/397 [00:00<00:00, 793.50frames/s]\n","100%|██████████| 382/382 [00:00<00:00, 2950.57frames/s]\n","100%|██████████| 389/389 [00:00<00:00, 1864.41frames/s]\n","100%|██████████| 384/384 [00:00<00:00, 1778.72frames/s]\n","100%|██████████| 430/430 [00:00<00:00, 2335.16frames/s]\n","100%|██████████| 340/340 [00:00<00:00, 1794.03frames/s]\n","100%|██████████| 358/358 [00:00<00:00, 1429.94frames/s]\n","100%|██████████| 280/280 [00:00<00:00, 1139.14frames/s]\n","100%|██████████| 310/310 [00:00<00:00, 1735.45frames/s]\n","100%|██████████| 332/332 [00:00<00:00, 1637.84frames/s]\n","100%|██████████| 394/394 [00:00<00:00, 2009.22frames/s]\n","100%|██████████| 355/355 [00:00<00:00, 1738.07frames/s]\n","100%|██████████| 489/489 [00:00<00:00, 2441.43frames/s]\n","100%|██████████| 386/386 [00:00<00:00, 1926.42frames/s]\n","100%|██████████| 319/319 [00:00<00:00, 1711.38frames/s]\n","100%|██████████| 406/406 [00:00<00:00, 2015.35frames/s]\n","100%|██████████| 410/410 [00:00<00:00, 1662.17frames/s]\n","100%|██████████| 449/449 [00:00<00:00, 1424.53frames/s]\n","100%|██████████| 303/303 [00:00<00:00, 1673.23frames/s]\n","100%|██████████| 386/386 [00:00<00:00, 1977.30frames/s]\n","100%|██████████| 417/417 [00:00<00:00, 1651.54frames/s]\n","100%|██████████| 426/426 [00:00<00:00, 1998.30frames/s]\n","100%|██████████| 411/411 [00:00<00:00, 2105.55frames/s]\n","100%|██████████| 435/435 [00:00<00:00, 2366.57frames/s]\n","100%|██████████| 452/452 [00:00<00:00, 1607.54frames/s]\n","100%|██████████| 425/425 [00:00<00:00, 1236.21frames/s]\n","100%|██████████| 348/348 [00:00<00:00, 1913.66frames/s]\n","100%|██████████| 422/422 [00:00<00:00, 1891.85frames/s]\n","100%|██████████| 391/391 [00:00<00:00, 1783.21frames/s]\n","100%|██████████| 360/360 [00:00<00:00, 2116.85frames/s]\n","100%|██████████| 376/376 [00:00<00:00, 2124.96frames/s]\n","100%|██████████| 355/355 [00:00<00:00, 2163.42frames/s]\n","100%|██████████| 397/397 [00:00<00:00, 2012.23frames/s]\n","100%|██████████| 425/425 [00:00<00:00, 1925.84frames/s]\n","100%|██████████| 310/310 [00:00<00:00, 1133.91frames/s]\n","100%|██████████| 376/376 [00:00<00:00, 1467.40frames/s]\n","100%|██████████| 415/415 [00:00<00:00, 2120.87frames/s]\n","100%|██████████| 424/424 [00:00<00:00, 1959.59frames/s]\n","100%|██████████| 340/340 [00:00<00:00, 2042.83frames/s]\n","100%|██████████| 400/400 [00:00<00:00, 2023.55frames/s]\n","100%|██████████| 429/429 [00:00<00:00, 2568.12frames/s]\n","100%|██████████| 481/481 [00:00<00:00, 2131.96frames/s]\n","100%|██████████| 415/415 [00:00<00:00, 1769.56frames/s]\n"]},{"output_type":"stream","name":"stdout","text":["Audios totales     : 80\n","Procesados OK      : 80\n","Con error          : 0\n","Salida (JSON por audio): /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/outputs/json\n","JSON global        : /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/outputs/json/resultados_global.json\n","\n","Entidades por tipo (acumuladas):\n"," - LOC: 77\n"," - PER: 32\n"," - MISC: 23\n"," - ORG: 23\n"]}]},{"cell_type":"markdown","source":["## 6.Validación y resumen de resultados\n","\n","En este apartado revisamos de forma sistemática la calidad del procesamiento por lotes. Para ello:\n","- Leemos el `resultados_global.json` generado en la fase anterior.\n","- Calculamos un resumen por archivo: longitud de la transcripción y número de entidades por tipo (PER, ORG, LOC, MISC) y total.\n","- Generamos un recuento global de las entidades más frecuentes por tipo, con su frecuencia y una confianza media aproximada.\n","- Guardamos dos ficheros de apoyo en `outputs/csv/`: `resumen_transcripciones.csv` (una fila por audio) y `entidades_top.csv` (frecuencias por entidad y tipo).\n","\n","Estos resúmenes nos permiten detectar rápidamente audios con transcripción vacía o anómala, así como validar que el etiquetado de entidades es coherente con el contenido."],"metadata":{"id":"WONrCHmYLNes"}},{"cell_type":"code","source":["# Resumen de calidad y guardado de CSV auxiliares.\n","OUT_JSON_DIR = PATHS[\"out_json\"]\n","OUT_CSV_DIR  = PATHS[\"out_csv\"]\n","os.makedirs(OUT_CSV_DIR, exist_ok=True)\n","\n","GLOBAL_JSON = os.path.join(OUT_JSON_DIR, \"resultados_global.json\")\n","\n","# 1) Cargar resultados\n","with open(GLOBAL_JSON, \"r\", encoding=\"utf-8\") as f:\n","    resultados = json.load(f)\n","\n","# 2) Construir resumen por archivo\n","resumen_rows = []\n","vacios = 0\n","tipos_conocidos = (\"PER\", \"ORG\", \"LOC\", \"MISC\")\n","\n","for r in resultados:\n","    fname = r.get(\"archivo_audio\", \"\")\n","    texto = (r.get(\"transcripcion\") or \"\").strip()\n","    ents  = r.get(\"entidades\") or {}\n","    if texto == \"\":\n","        vacios += 1\n","\n","    # Conteos por tipo\n","    counts = {t: len(ents.get(t, [])) for t in tipos_conocidos}\n","    total  = sum(counts.values())\n","\n","    resumen_rows.append({\n","        \"archivo_audio\": fname,\n","        \"longitud_transcripcion\": len(texto),\n","        \"n_PER\": counts[\"PER\"],\n","        \"n_ORG\": counts[\"ORG\"],\n","        \"n_LOC\": counts[\"LOC\"],\n","        \"n_MISC\": counts[\"MISC\"],\n","        \"n_total\": total,\n","    })\n","\n","# Guardar resumen por archivo\n","resumen_csv = os.path.join(OUT_CSV_DIR, \"resumen_transcripciones.csv\")\n","with open(resumen_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n","    writer = csv.DictWriter(\n","        f,\n","        fieldnames=[\n","            \"archivo_audio\", \"longitud_transcripcion\",\n","            \"n_PER\", \"n_ORG\", \"n_LOC\", \"n_MISC\", \"n_total\"\n","        ],\n","    )\n","    writer.writeheader()\n","    writer.writerows(resumen_rows)\n","\n","# 3) Frecuencias de entidades por tipo y texto + confianza media\n","freq = defaultdict(lambda: {\"count\": 0, \"score_sum\": 0.0})\n","for r in resultados:\n","    ents = r.get(\"entidades\") or {}\n","    for t, lista in ents.items():\n","        for e in lista:\n","            clave = (t, e.get(\"texto\", \"\"))\n","            freq[clave][\"count\"] += 1\n","            freq[clave][\"score_sum\"] += float(e.get(\"score\", 0.0))\n","\n","# Preparar filas ordenadas por tipo y frecuencia\n","entidades_rows = []\n","for (t, texto), info in freq.items():\n","    avg_score = info[\"score_sum\"] / max(info[\"count\"], 1)\n","    entidades_rows.append({\n","        \"tipo\": t,\n","        \"texto\": texto,\n","        \"frecuencia\": info[\"count\"],\n","        \"confianza_media\": round(avg_score, 3),\n","    })\n","\n","# Orden: tipo asc, frecuencia desc, texto asc\n","entidades_rows.sort(key=lambda x: (x[\"tipo\"], -x[\"frecuencia\"], x[\"texto\"]))\n","\n","# Guardar CSV de entidades top\n","entidades_csv = os.path.join(OUT_CSV_DIR, \"entidades_top.csv\")\n","with open(entidades_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n","    writer = csv.DictWriter(\n","        f,\n","        fieldnames=[\"tipo\", \"texto\", \"frecuencia\", \"confianza_media\"],\n","    )\n","    writer.writeheader()\n","    writer.writerows(entidades_rows)\n","\n","# 4) Informe corto en consola\n","total = len(resumen_rows)\n","print(\"Audios procesados  :\", total)\n","print(\"Transcripciones vacías:\", vacios)\n","print(\"CSV (resumen por archivo):\", resumen_csv)\n","print(\"CSV (entidades top):      \", entidades_csv)\n","\n","# Mostrar un avance pequeño (primeras 5 filas del resumen)\n","print(\"\\nPrimeras 5 filas del resumen:\")\n","for row in resumen_rows[:5]:\n","    print(row)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SX2diVLWLON8","executionInfo":{"status":"ok","timestamp":1755429348058,"user_tz":-120,"elapsed":795,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"983c5a3e-064a-4f4b-b00d-5e8ed7b8532a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Audios procesados  : 80\n","Transcripciones vacías: 0\n","CSV (resumen por archivo): /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/outputs/csv/resumen_transcripciones.csv\n","CSV (entidades top):       /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/outputs/csv/entidades_top.csv\n","\n","Primeras 5 filas del resumen:\n","{'archivo_audio': 'frase_01.wav', 'longitud_transcripcion': 61, 'n_PER': 1, 'n_ORG': 0, 'n_LOC': 1, 'n_MISC': 1, 'n_total': 3}\n","{'archivo_audio': 'frase_02.wav', 'longitud_transcripcion': 53, 'n_PER': 1, 'n_ORG': 0, 'n_LOC': 1, 'n_MISC': 0, 'n_total': 2}\n","{'archivo_audio': 'frase_03.wav', 'longitud_transcripcion': 56, 'n_PER': 0, 'n_ORG': 1, 'n_LOC': 1, 'n_MISC': 1, 'n_total': 3}\n","{'archivo_audio': 'frase_04.wav', 'longitud_transcripcion': 54, 'n_PER': 0, 'n_ORG': 0, 'n_LOC': 1, 'n_MISC': 1, 'n_total': 2}\n","{'archivo_audio': 'frase_05.wav', 'longitud_transcripcion': 63, 'n_PER': 0, 'n_ORG': 0, 'n_LOC': 2, 'n_MISC': 0, 'n_total': 2}\n"]}]},{"cell_type":"markdown","source":["## 7.Conclusiones\n","\n","Hemos transcrito todos los audios con Whisper y aplicado NER en español con `MMG/xlm-roberta-large-ner-spanish`. Generamos un **JSON por audio** y un **JSON global** con el nombre del archivo, la transcripción y las entidades agrupadas por tipo, tal y como exige el enunciado. La validación final confirma que no hubo errores y que las transcripciones no quedaron vacías.\n","Además, guardamos dos **CSV de apoyo**:\n","- `resumen_transcripciones.csv`: recoge, para cada audio, la longitud de la transcripción y el número de entidades detectadas por tipo (PER, ORG, LOC, MISC) y el total. Nos sirve para revisar rápidamente posibles casos anómalos.\n","- `entidades_top.csv`: lista cada entidad detectada (por texto y tipo), su **frecuencia** y la **confianza media**. Nos ayuda a entender qué entidades aparecen más y con qué seguridad.\n","\n","Como mejoras futuras, podríamos evaluar tamaños superiores de Whisper para ganar precisión, revisar entidades etiquetadas como MISC y normalizar nombres propios para análisis posteriores."],"metadata":{"id":"TEX7dna0OMmY"}},{"cell_type":"markdown","source":["## 8.Despliegue con Gradio\n","\n","En este apartado montamos una **mini app web** con **Gradio** para probar el flujo de **transcripción** (Whisper) y **reconocimiento de entidades** (XLM-R).  \n","La interfaz permite subir un audio (o grabarlo con el micro), lanzar la transcripción y visualizar una tabla con las entidades detectadas.\n","\n","**Objetivos:**\n","- Facilitar la **demostración** del modelo a no técnicos.\n","- Generar **evidencias** (JSON por ejecución) sin reentrenar ni alterar el pipeline.\n","\n","**Metodología y banderas:**\n","- Respetamos `USE_DRIVE` para rutas.\n","- Añadimos `SAVE_RESULTS` para **guardar** o no las salidas de la app (por defecto, activado).\n","- Si los modelos (`whisper_model`, `ner_pipe`) están ya en memoria, **se reutilizan**; en caso contrario, se **cargan** con la misma configuración del cuaderno.\n","\n","**Salidas (si `SAVE_RESULTS=True`):**\n","- Un JSON por ejecución en `outputs/json/gradio_runs/` con transcripción, entidades y metadatos.\n","- Guardamos las capturas de pantalla en `outputs/figures/`\n","\n","> Preparamos el despliegue con Gradio de forma autónoma, de modo que puede ejecutarse sin necesidad de rehacer todos los pasos previos pero manteniendo la coherencia con los modelos utilizados en el ejercicio.\n","> Está optimizado para su ejecución en Google Colab.\n","> De esta forma se genera automáticamente un enlace público temporal (`.gradio.live`) accesible desde cualquier navegador.\n","> En un entorno local (Jupyter Notebook), bastaría con instalar Gradio y ejecutar las mismas celdas para abrir la interfaz en `http://127.0.0.1:7860`."],"metadata":{"id":"hme0ggVIlJq1"}},{"cell_type":"code","source":["# Flag para controlar si lanzamos realmente el despliegue o solo cargamos resultados ya generados\n","RUN_DEPLOY = True"],"metadata":{"id":"J2gU2VnZrrH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Configuración para el despliegue con Gradio ===\n","from pathlib import Path\n","import json, os\n","from datetime import datetime\n","\n","# Directorios\n","OUT_JSON_DIR = Path(PATHS[\"out_json\"])\n","GRADIO_RUNS  = OUT_JSON_DIR / \"gradio_runs\"\n","GRADIO_RUNS.mkdir(parents=True, exist_ok=True)\n","\n","# Seleccionamos una muestra real para el modo entregable\n","candidatos = sorted(GRADIO_RUNS.glob(\"*.json\"))\n","if len(candidatos) == 0:\n","    ejemplo = {\n","        \"archivo_audio\": \"ejemplo.wav\",\n","        \"transcripcion\": \"Buenos días, esto es un ejemplo de transcripción.\",\n","        \"entidades\": {\"LOC\": [{\"texto\": \"Madrid\", \"score\": 0.98}]},\n","        \"modelo_whisper\": \"base\",\n","        \"modelo_ner\": \"MMG/xlm-roberta-large-ner-spanish\",\n","        \"timestamp\": datetime.utcnow().isoformat()+\"Z\",\n","    }\n","    sample_path = GRADIO_RUNS / \"sample_gradio.json\"\n","    with open(sample_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(ejemplo, f, ensure_ascii=False, indent=2)\n","    print(\"ℹ️ No se encontraron JSON previos. Creado ejemplo sintético en:\", sample_path)\n","else:\n","    src = candidatos[0]\n","    sample_path = GRADIO_RUNS / \"sample_gradio.json\"\n","    with open(src, \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","    ejemplo = {\n","        \"archivo_audio\": data.get(\"archivo_audio\", os.path.basename(data.get(\"ruta_audio\",\"muestra.wav\"))),\n","        \"transcripcion\": data.get(\"transcripcion\", data.get(\"texto\",\"\")),\n","        \"entidades\": data.get(\"entidades\", {}),\n","        \"modelo_whisper\": \"base\",\n","        \"modelo_ner\": \"MMG/xlm-roberta-large-ner-spanish\",\n","        \"timestamp\": datetime.utcnow().isoformat()+\"Z\",\n","    }\n","    with open(sample_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(ejemplo, f, ensure_ascii=False, indent=2)\n","    print(\"✅ Muestra real preparada para el modo entregable:\", sample_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mInWhdjbmyqt","executionInfo":{"status":"ok","timestamp":1755430208510,"user_tz":-120,"elapsed":34,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"ef693f4d-a1dc-4203-9bc6-ab6592562028"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Muestra real preparada para el modo entregable: /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/outputs/json/gradio_runs/sample_gradio.json\n"]}]},{"cell_type":"code","source":["# === 8.B) Despliegue con Gradio (limpio, integrado con PATHS/GRADIO_RUNS) ===\n","# Usa RUN_DEPLOY:\n","#   - True  → instala condicionalmente, carga modelos y lanza Gradio (en Colab: share=True)\n","#   - False → modo entregable: muestra la muestra real 'sample_gradio.json' (sin instalar nada)\n","\n","# Comprobación por si RUN_DEPLOY no existe:\n","if \"RUN_DEPLOY\" not in globals():\n","    RUN_DEPLOY = False\n","\n","if RUN_DEPLOY:\n","    # --- Instalación condicional de dependencias pesadas ---\n","    try:\n","        import whisper\n","    except ImportError:\n","        %pip -q install -U openai-whisper\n","        import whisper\n","\n","    try:\n","        import gradio as gr\n","    except ImportError:\n","        %pip -q install -U gradio\n","        import gradio as gr\n","\n","    try:\n","        import transformers\n","        from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n","    except ImportError:\n","        %pip -q install -U transformers accelerate\n","        from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n","\n","    import shutil\n","    if not shutil.which(\"ffmpeg\"):\n","        !apt -q install -y ffmpeg\n","\n","    # --- Imports específicos para el despliegue ---\n","    import torch\n","    import pandas as pd\n","\n","    # --- Config coherente con pipeline (si existen, reusa; si no, default) ---\n","    device       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    WHISPER_SIZE = globals().get(\"WHISPER_SIZE\", \"base\")\n","    NER_MODEL    = globals().get(\"NER_MODEL\", \"MMG/xlm-roberta-large-ner-spanish\")\n","\n","    # --- Carga perezosa de modelos (si ya estaban en memoria, se reutilizan) ---\n","    if \"whisper_model\" not in globals():\n","        whisper_model = whisper.load_model(WHISPER_SIZE, device=device)\n","\n","    if \"ner_pipe\" not in globals():\n","        tokenizer = AutoTokenizer.from_pretrained(NER_MODEL)\n","        ner_model = AutoModelForTokenClassification.from_pretrained(NER_MODEL)\n","        ner_pipe  = pipeline(\n","            \"token-classification\",\n","            model=ner_model,\n","            tokenizer=tokenizer,\n","            aggregation_strategy=\"simple\",\n","            device=0 if device == \"cuda\" else -1\n","        )\n","\n","    # --- Auxiliar: convertir entidades a DataFrame legible ---\n","    def _ents_to_df(ents):\n","        if not ents:\n","            return pd.DataFrame(columns=[\"entity_group\",\"word\",\"score\",\"start\",\"end\"])\n","        rows = []\n","        for e in ents:\n","            rows.append({\n","                \"entity_group\": e.get(\"entity_group\",\"\"),\n","                \"word\": e.get(\"word\",\"\"),\n","                \"score\": float(e.get(\"score\",0.0)),\n","                \"start\": e.get(\"start\", None),\n","                \"end\": e.get(\"end\", None),\n","            })\n","        return pd.DataFrame(rows)[[\"entity_group\",\"word\",\"score\",\"start\",\"end\"]]\n","\n","    # --- Lógica de la app: transcribe y hace NER; guarda JSON por ejecución ---\n","    def transcribir_y_ner(audio_path: str, language: str = \"es\"):\n","        if not audio_path or not os.path.exists(audio_path):\n","            return \"⚠️ No se recibió audio.\", pd.DataFrame(columns=[\"entity_group\",\"word\",\"score\",\"start\",\"end\"])\n","\n","        # 1) Transcripción\n","        result = whisper_model.transcribe(audio_path, language=language, verbose=False)\n","        texto  = (result.get(\"text\") or \"\").strip()\n","\n","        # 2) NER\n","        ents   = ner_pipe(texto) if texto else []\n","        df     = _ents_to_df(ents)\n","\n","        # 3) Guardado JSON de esta ejecución en outputs/json/gradio_runs\n","        payload = {\n","            \"timestamp_utc\": datetime.utcnow().isoformat()+\"Z\",\n","            \"audio_file\": os.path.basename(audio_path),\n","            \"language\": language,\n","            \"whisper_size\": WHISPER_SIZE,\n","            \"ner_model\": NER_MODEL,\n","            \"transcripcion\": texto,\n","            \"entidades\": df.to_dict(orient=\"records\"),\n","        }\n","        run_name = f\"gradio_run_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\"\n","        with open((Path(PATHS[\"out_json\"]) / \"gradio_runs\" / run_name), \"w\", encoding=\"utf-8\") as f:\n","            json.dump(payload, f, ensure_ascii=False, indent=2)\n","\n","        return texto, df\n","\n","    # --- Interfaz Gradio ---\n","    with gr.Blocks(title=\"Transcripción + NER (Whisper + XLM-R)\") as demo:\n","        gr.Markdown(\"## 🎙️ Transcripción y entidades\\nSube un audio en español y obtén el texto y las entidades extraídas.\")\n","        with gr.Row():\n","            audio_in = gr.Audio(sources=[\"upload\",\"microphone\"], type=\"filepath\", label=\"Audio (wav/mp3/m4a)\")\n","            lang_in  = gr.Dropdown(choices=[\"es\",\"en\",\"fr\",\"de\",\"it\",\"pt\"], value=\"es\", label=\"Idioma\")\n","        btn     = gr.Button(\"Transcribir\")\n","        txt_out = gr.Textbox(label=\"Transcripción\", lines=8)\n","        ents_df = gr.Dataframe(label=\"Entidades (NER)\", interactive=False)\n","\n","        btn.click(transcribir_y_ner, inputs=[audio_in, lang_in], outputs=[txt_out, ents_df])\n","\n","    # En Colab, share=True para URL pública temporal (.gradio.live)\n","    demo.launch(share=True)\n","\n","else:\n","    # === MODO ENTREGABLE: muestra sample_gradio.json creado en 8.A (sin instalar nada) ===\n","    sample_path = Path(PATHS[\"out_json\"]) / \"gradio_runs\" / \"sample_gradio.json\"\n","    if sample_path.exists():\n","        with open(sample_path, \"r\", encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","        print(\"📄 Ejemplo real (modo entregable):\")\n","        print(json.dumps(data, ensure_ascii=False, indent=2))\n","    else:\n","        print(\"⚠️ No se encontró sample_gradio.json en\", sample_path.parent)\n"],"metadata":{"id":"MgHOVCkM5XS2","colab":{"base_uri":"https://localhost:8080/","height":612},"executionInfo":{"status":"ok","timestamp":1755430877852,"user_tz":-120,"elapsed":2427,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"10e9e245-2abd-46a2-e2a0-3c80dfa3127e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://124c5cf105481c87e6.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://124c5cf105481c87e6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"omAU3Z-6pOQt"},"execution_count":null,"outputs":[]}]}