{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[{"file_id":"1z_w1KivVCV4R_QeMvAtoaN7sZVabO3Gl","timestamp":1754743815471}],"gpuType":"T4","authorship_tag":"ABX9TyNC0KmzWkD0yr6bay9gUISW"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["# Ejercicio 1 ‚Äî Transcripci√≥n de audios y extracci√≥n de entidades (NER)\n","\n","En este ejercicio vamos a resolver el problema en dos etapas: primero convertimos los audios en espa√±ol a texto mediante **Whisper** y, despu√©s, aplicamos **NER** con el modelo preentrenado `MMG/xlm-roberta-large-ner-spanish` para detectar entidades como personas (PER), organizaciones (ORG), ubicaciones (LOC), fechas (DATE), etc. La salida solicitada por el enunciado ser√° un **JSON por audio** y un **JSON global** con el identificador del archivo, la transcripci√≥n y las entidades agrupadas por tipo.\n","\n","**Pasos a seguir**\n","1. Preparaci√≥n del entorno (Colab + Drive + rutas)\n","2. Verificaci√≥n del entorno (Python y GPU)\n","3. Instalaci√≥n de dependencias (Whisper y Transformers)  \n","4. Prueba piloto con un audio  \n","5. Procesamiento por lotes y guardado de resultados (JSON)  \n","6. Validaci√≥n r√°pida y resumen  "],"metadata":{"id":"DBRrfIno6tBX"}},{"cell_type":"markdown","source":["## 1.Rutas, estructura y README\n","\n","Trabajamos habitualmente en **Google Drive** para guardar y reanudar f√°cilmente el ejercicio (`USE_DRIVE=True`).  \n","Para la **entrega**, usamos **rutas relativas** con `USE_DRIVE=False`, de modo que el cuaderno sea **portable** y se pueda **re-ejecutar** sin depender de Drive. El cuaderno es **reproducible**: al volver a ejecutarlo, transcribe de nuevo y regenera los JSON/CSV.\n","\n","Estructura creada:\n","- `notebooks/`, `src/`, `scripts/` (aqu√≠ dejaremos el **.py** exportado)\n","- `data/audio/` (aqu√≠ se copian los **audios**; no van en el ZIP)\n","- `outputs/json/`, `outputs/csv/`, `outputs/figures/`\n","- `outputs/docs/` (aqu√≠ guardaremos el **PDF** del informe)\n","- `outputs/logs/`\n","\n","El README indica d√≥nde colocar **datos**, **.py** y **PDF**, y qu√© salidas se generan."],"metadata":{"id":"JVTwidoM7fq6"}},{"cell_type":"code","source":["#Rutas + estructura + README\n","from pathlib import Path\n","\n","USE_DRIVE = False  #True (monta Drive)\n","                   #False (rutas relativas, sin Drive)\n","\n","if USE_DRIVE:\n","    try:\n","        from google.colab import drive  # solo si estamos en Colab\n","        drive.mount('/content/drive')\n","    except Exception as e:\n","        print(\"Aviso: no se pudo montar Drive (¬øno est√°s en Colab?).\", e)\n","    ROOT = Path(\"/content/drive/MyDrive/MASTER BIG DATA/md2_2025\")\n","    EJ1  = ROOT / \"ej1_transcripcion\"\n","else:\n","    here = Path(\".\").resolve()\n","    EJ1  = here if (here.name == \"ej1_transcripcion\") else (here / \"ej1_transcripcion\")\n","\n","PATHS = {\n","    \"root\":        str(EJ1.parent),\n","    \"ej1\":         str(EJ1),\n","    \"nb\":          str(EJ1 / \"notebooks\"),\n","    \"src\":         str(EJ1 / \"src\"),\n","    \"scripts\":     str(EJ1 / \"scripts\"),                 # .py exportado\n","    \"data_audio\":  str(EJ1 / \"data\" / \"audio\"),\n","    \"out_json\":    str(EJ1 / \"outputs\" / \"json\"),\n","    \"out_csv\":     str(EJ1 / \"outputs\" / \"csv\"),\n","    \"out_figs\":    str(EJ1 / \"outputs\" / \"figures\"),\n","    \"out_docs\":    str(EJ1 / \"outputs\" / \"docs\"),        # PDF del informe\n","    \"out_logs\":    str(EJ1 / \"outputs\" / \"logs\"),\n","}\n","\n","def ensure_dir(p: str): Path(p).mkdir(parents=True, exist_ok=True)\n","for k in (\"nb\",\"src\",\"scripts\",\"data_audio\",\"out_json\",\"out_csv\",\"out_figs\",\"out_docs\",\"out_logs\"):\n","    ensure_dir(PATHS[k])\n","\n","# README\n","readme = Path(PATHS[\"ej1\"]) / \"README.md\"\n","if not readme.exists():\n","    readme.write_text(\"\"\"# Ejercicio 1 ‚Äî Transcripci√≥n + NER (ES)\n","\n","**Objetivo.** Transcribimos audios en espa√±ol con Whisper y extraemos entidades (PER/ORG/LOC/MISC) con `MMG/xlm-roberta-large-ner-spanish`.\n","**Notebook principal:** `notebooks/ej1_transcripcion.ipynb`\n","\n","## Datos (no incluidos en el ZIP)\n","Copiar los audios en: `data/audio/` (formatos: wav, mp3, m4a, flac, ogg).\n","\n","## Salidas\n","- `outputs/json/`  ‚Üí JSON por audio + `resultados_global.json`\n","- `outputs/csv/`   ‚Üí res√∫menes tabulares (si se generan)\n","- `outputs/figures/` ‚Üí figuras opcionales (si se generan)\n","\n","## Entrega\n","- Exportar el notebook a **PDF** y guardarlo en `outputs/docs/`.\n","- Exportar a **.py** y colocarlo en `scripts/`.\n","\n","## Requisitos (Colab o local)\n","- `ffmpeg`, `openai-whisper`, `transformers`, `torch` (y aceleraci√≥n si hay GPU).\n","\"\"\", encoding=\"utf-8\")\n","    print(\"README creado:\", readme)\n","else:\n","    print(\"README ya existe: no se modifica.\")\n","\n","print(\"Rutas clave:\")\n","for k in (\"data_audio\",\"out_json\",\"out_csv\",\"out_figs\",\"out_docs\",\"scripts\"):\n","    print(f\"  {k:>11} ‚Üí {PATHS[k]}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nDIrv6KA7iLd","executionInfo":{"status":"ok","timestamp":1755504278466,"user_tz":-120,"elapsed":17,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"38f418e8-4431-44dd-a39f-95b1d989fbe0"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["README creado: /content/ej1_transcripcion/README.md\n","Rutas clave:\n","   data_audio ‚Üí /content/ej1_transcripcion/data/audio\n","     out_json ‚Üí /content/ej1_transcripcion/outputs/json\n","      out_csv ‚Üí /content/ej1_transcripcion/outputs/csv\n","     out_figs ‚Üí /content/ej1_transcripcion/outputs/figures\n","     out_docs ‚Üí /content/ej1_transcripcion/outputs/docs\n","      scripts ‚Üí /content/ej1_transcripcion/scripts\n"]}]},{"cell_type":"markdown","source":["## 2.Verificaci√≥n del entorno (Python y GPU)\n","\n","Comprobamos la versi√≥n de Python y si disponemos de GPU. Esta informaci√≥n nos ayudar√° a decidir si ejecutamos los modelos en CPU o en GPU en los siguientes pasos."],"metadata":{"id":"bYtv9KsO94xx"}},{"cell_type":"code","source":["# Inspecci√≥n r√°pida del entorno\n","\n","import platform\n","import torch\n","\n","print(\"Python:\", platform.python_version())\n","has_cuda = torch.cuda.is_available()\n","print(\"Torch CUDA disponible:\", has_cuda)\n","if has_cuda:\n","    print(\"GPU:\", torch.cuda.get_device_name(0))\n","else:\n","    print(\"Sin GPU.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NjBHfMaK91hS","executionInfo":{"status":"ok","timestamp":1755429136818,"user_tz":-120,"elapsed":26,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"b6950f61-fc42-437c-b24c-12bb7204b903"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python: 3.11.13\n","Torch CUDA disponible: True\n","GPU: Tesla T4\n"]}]},{"cell_type":"markdown","source":["## 3.Instalaci√≥n de dependencias\n","\n","Instalamos las librer√≠as necesarias para el ejercicio: **Whisper** (transcripci√≥n), **Transformers** (NER) y **ffmpeg** (necesario para procesar audio). Mantendremos las versiones provistas por Colab para maximizar la compatibilidad."],"metadata":{"id":"_exvTQWR-8hh"}},{"cell_type":"code","source":["# Instalaci√≥n de librer√≠as necesarias\n","!pip -q install -U openai-whisper transformers accelerate datasets\n","!apt -q install -y ffmpeg"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"J64jgOSl9959","executionInfo":{"status":"ok","timestamp":1755429152028,"user_tz":-120,"elapsed":15223,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"70314b7f-fa9b-4167-b65f-d9db4cf425d9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m42.0/42.0 kB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m11.3/11.3 MB\u001b[0m \u001b[31m116.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hReading package lists...\n","Building dependency tree...\n","Reading state information...\n","ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n","0 upgraded, 0 newly installed, 0 to remove and 35 not upgraded.\n"]}]},{"cell_type":"code","source":["### Comprobaci√≥n de la instalaci√≥n\n","import sys\n","import transformers, whisper\n","\n","print(\"Python      :\", platform.python_version())\n","print(\"Torch       :\", torch.__version__)\n","print(\"Transformers:\", transformers.__version__)\n","# confirmamos import y ruta de instalaci√≥n\n","print(\"Whisper     : OK (\", whisper.__file__, \")\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"kLK9ZSPBABWj","executionInfo":{"status":"ok","timestamp":1755429152082,"user_tz":-120,"elapsed":53,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"95d49cc8-8d45-4981-9b0a-d500b1541479"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Python      : 3.11.13\n","Torch       : 2.6.0+cu124\n","Transformers: 4.55.1\n","Whisper     : OK ( /usr/local/lib/python3.11/dist-packages/whisper/__init__.py )\n"]}]},{"cell_type":"code","source":["#Importaci√≥n resto de librerias\n","# Est√°ndar\n","import glob, json, csv\n","from datetime import datetime\n","from collections import Counter, defaultdict\n","\n","# Terceros\n","from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n"],"metadata":{"id":"FcRfhIUCSMyh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## 4.Carga de audios de prueba\n","\n","Listamos los audios disponibles en `ej1_transcripcion/data/audio/` y seleccionamos autom√°ticamente el primero como muestra de trabajo. Este paso nos permite verificar que la carpeta de datos est√° correctamente configurada antes de continuar."],"metadata":{"id":"p_hkX-HGAKfd"}},{"cell_type":"code","source":["AUDIO_DIR = PATHS[\"data_audio\"]\n","\n","# Listamos extensiones comunes de audio por si en un futuro a√±adimos mas archivos\n","exts = (\"*.wav\", \"*.mp3\", \"*.m4a\", \"*.flac\", \"*.ogg\")\n","audio_paths = []\n","for ext in exts:\n","    audio_paths.extend(glob.glob(os.path.join(AUDIO_DIR, ext)))\n","\n","audio_paths = sorted(audio_paths)\n","\n","print(\"Carpeta de audio:\", AUDIO_DIR)\n","print(\"N¬∫ de archivos encontrados:\", len(audio_paths))\n","for p in audio_paths[:10]:\n","    print(\" -\", os.path.basename(p))\n","\n","# Seleccionamos un audio de ejemplo\n","AUDIO_SAMPLE = audio_paths[0] if audio_paths else None\n","print(\"Audio de ejemplo seleccionado:\", AUDIO_SAMPLE)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"g_wrwyFfEeJq","executionInfo":{"status":"ok","timestamp":1755429152095,"user_tz":-120,"elapsed":5,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"efaab997-04f1-4872-8bde-09f193c981b1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Carpeta de audio: /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/data/audio\n","N¬∫ de archivos encontrados: 80\n"," - frase_01.wav\n"," - frase_02.wav\n"," - frase_03.wav\n"," - frase_04.wav\n"," - frase_05.wav\n"," - frase_06.wav\n"," - frase_07.wav\n"," - frase_08.wav\n"," - frase_09.wav\n"," - frase_10.wav\n","Audio de ejemplo seleccionado: /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/data/audio/frase_01.wav\n"]}]},{"cell_type":"markdown","source":["### 4.1Prueba piloto con un audio\n","\n","En esta secci√≥n cargamos los modelos necesarios, transcribimos el audio de ejemplo y ejecutamos el reconocimiento de entidades (NER). Despu√©s agrupamos las entidades por tipo y construimos un diccionario con el formato de salida que utilizaremos m√°s adelante (JSON), manteni√©ndolo de momento en memoria para validar el flujo.\n","\n","**Pasos:**\n","1. Cargar Whisper (tama√±o `base`) y el modelo NER `MMG/xlm-roberta-large-ner-spanish`.\n","2. Transcribir el archivo de audio seleccionado autom√°ticamente.\n","3. Ejecutar NER sobre la transcripci√≥n.\n","4. Agrupar las entidades por tipo y mostrar un resumen (conteos por tipo y primeros ejemplos)."],"metadata":{"id":"e7BdtQMIGdDI"}},{"cell_type":"code","source":["# 1) Dispositivo\n","device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","\n","# 2) Modelos\n","WHISPER_SIZE = \"base\"  # se podr√° cambiar a 'small'/'medium' m√°s adelante si lo necesitamos\n","NER_MODEL = \"MMG/xlm-roberta-large-ner-spanish\"\n","\n","whisper_model = whisper.load_model(WHISPER_SIZE, device=device)\n","\n","tokenizer = AutoTokenizer.from_pretrained(NER_MODEL)\n","ner_model = AutoModelForTokenClassification.from_pretrained(NER_MODEL)\n","ner_pipe = pipeline(\n","    \"token-classification\",\n","    model=ner_model,\n","    tokenizer=tokenizer,\n","    aggregation_strategy=\"simple\",\n","    device=0 if device == \"cuda\" else -1\n",")\n","\n","# 3) Audio de ejemplo (seleccionado en la secci√≥n anterior)\n","audio_path = AUDIO_SAMPLE\n","fname = os.path.basename(audio_path) if audio_path else None\n","\n","# 4) Transcripci√≥n\n","transcripcion = \"\"\n","if audio_path:\n","    result = whisper_model.transcribe(audio_path, language=\"es\", verbose=False)\n","    transcripcion = (result.get(\"text\") or \"\").strip()\n","\n","# 5) NER\n","entidades_por_tipo = {}\n","if transcripcion:\n","    ents = ner_pipe(transcripcion)\n","    for ent in ents:\n","        etype = ent.get(\"entity_group\", \"MISC\")\n","        item = {\n","            \"texto\": ent.get(\"word\", \"\"),\n","            \"start\": int(ent.get(\"start\", 0)),\n","            \"end\": int(ent.get(\"end\", 0)),\n","            \"score\": float(ent.get(\"score\", 0.0)),\n","        }\n","        entidades_por_tipo.setdefault(etype, []).append(item)\n","\n","# 6) Resultado piloto en memoria (formato final que usaremos para JSON)\n","resultado_piloto = {\n","    \"archivo_audio\": fname,\n","    \"ruta_audio\": audio_path,\n","    \"transcripcion\": transcripcion,\n","    \"entidades\": entidades_por_tipo,\n","    \"modelo_whisper\": WHISPER_SIZE,\n","    \"modelo_ner\": NER_MODEL,\n","    \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n","}\n","\n","# 7) Resumen legible\n","print(\"Archivo:\", resultado_piloto[\"archivo_audio\"])\n","print(\"\\nTranscripci√≥n (primeros 200 caracteres):\")\n","print(resultado_piloto[\"transcripcion\"][:200])\n","\n","print(\"\\nEntidades detectadas por tipo:\")\n","for t, lista in resultado_piloto[\"entidades\"].items():\n","    print(f\" - {t}: {len(lista)}\")\n","\n","# 8) Muestra de las primeras 3 entidades de cada tipo (si existen)\n","for t, lista in resultado_piloto[\"entidades\"].items():\n","    if not lista:\n","        continue\n","    print(f\"\\nEjemplos [{t}]:\")\n","    for e in lista[:3]:\n","        print(f\"  ‚Ä¢ {e['texto']} (score={e['score']:.3f})\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jtsaXAWkGj5X","executionInfo":{"status":"ok","timestamp":1755429172379,"user_tz":-120,"elapsed":20283,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"115e0ccb-206b-4c03-cd37-3a87588f6019"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cuda:0\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 405/405 [00:02<00:00, 181.94frames/s]\n"]},{"output_type":"stream","name":"stdout","text":["Archivo: frase_01.wav\n","\n","Transcripci√≥n (primeros 200 caracteres):\n","Sofia viaj√≥ a Sevilla para asistir a un Congreso de Medicina.\n","\n","Entidades detectadas por tipo:\n"," - PER: 1\n"," - LOC: 1\n"," - MISC: 1\n","\n","Ejemplos [PER]:\n","  ‚Ä¢ Sofia (score=0.983)\n","\n","Ejemplos [LOC]:\n","  ‚Ä¢ Sevilla (score=0.999)\n","\n","Ejemplos [MISC]:\n","  ‚Ä¢ Congreso de Medicina (score=0.979)\n"]}]},{"cell_type":"markdown","source":["## 5.Procesamiento por lotes y guardado de resultados (JSON)\n","\n","Aplicamos el flujo validado a todos los audios de `ej1_transcripcion/data/audio/`. Para cada archivo generamos un **JSON individual** con: `archivo_audio`, `ruta_audio`, `transcripcion`, `entidades` (agrupadas por tipo), `modelo_whisper`, `modelo_ner` y `timestamp`. Adem√°s, consolidamos todo en un **JSON global**. Los guardados se realizan en `ej1_transcripcion/outputs/json/`."],"metadata":{"id":"D5AOAwaJJ6GH"}},{"cell_type":"code","source":["# --- Utilidades ---\n","def transcribir_audio(path_audio, model, language=\"es\"):\n","    \"\"\"Transcribe un audio con Whisper y devuelve el texto (str).\"\"\"\n","    out = model.transcribe(path_audio, language=language, verbose=False)\n","    return (out.get(\"text\") or \"\").strip()\n","\n","def extraer_entidades(texto, ner_pipeline):\n","    \"\"\"Ejecuta NER y agrupa por tipo. Devuelve dict {tipo: [..]}.\"\"\"\n","    if not texto:\n","        return {}\n","    raw = ner_pipeline(texto)\n","    entidades = {}\n","    for ent in raw:\n","        etype = ent.get(\"entity_group\", \"MISC\")\n","        item = {\n","            \"texto\": ent.get(\"word\", \"\"),\n","            \"start\": int(ent.get(\"start\", 0)),\n","            \"end\": int(ent.get(\"end\", 0)),\n","            \"score\": float(ent.get(\"score\", 0.0)),\n","        }\n","        entidades.setdefault(etype, []).append(item)\n","    return entidades\n","\n","# --- Garantizar modelos en memoria (por si la sesi√≥n se reinici√≥) ---\n","try:\n","    whisper_model\n","    ner_pipe\n","    WHISPER_SIZE\n","    NER_MODEL\n","except NameError:\n","    import torch, whisper\n","    from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n","    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    WHISPER_SIZE = \"base\"\n","    NER_MODEL = \"MMG/xlm-roberta-large-ner-spanish\"\n","    whisper_model = whisper.load_model(WHISPER_SIZE, device=device)\n","    tok = AutoTokenizer.from_pretrained(NER_MODEL)\n","    ner_model = AutoModelForTokenClassification.from_pretrained(NER_MODEL)\n","    ner_pipe = pipeline(\n","        \"token-classification\",\n","        model=ner_model,\n","        tokenizer=tok,\n","        aggregation_strategy=\"simple\",\n","        device=0 if device == \"cuda\" else -1\n","    )\n","\n","# --- Localizar audios y preparar salida ---\n","AUDIO_DIR = PATHS[\"data_audio\"]\n","OUT_JSON_DIR = PATHS[\"out_json\"]\n","os.makedirs(OUT_JSON_DIR, exist_ok=True)\n","\n","exts = (\"*.wav\", \"*.mp3\", \"*.m4a\", \"*.flac\", \"*.ogg\")\n","audio_paths = []\n","for ext in exts:\n","    audio_paths.extend(glob.glob(os.path.join(AUDIO_DIR, ext)))\n","audio_paths = sorted(audio_paths)\n","\n","resultados = []\n","contador_tipos = Counter()\n","procesados = 0\n","fallidos = 0\n","\n","# --- Bucle principal ---\n","for apath in audio_paths:\n","    fname = os.path.basename(apath)\n","    try:\n","        # 1) Transcripci√≥n\n","        texto = transcribir_audio(apath, whisper_model, language=\"es\")\n","\n","        # 2) NER\n","        entidades = extraer_entidades(texto, ner_pipe)\n","\n","        # 3) Registro y guardado JSON individual\n","        registro = {\n","            \"archivo_audio\": fname,\n","            \"ruta_audio\": apath,\n","            \"transcripcion\": texto,\n","            \"entidades\": entidades,\n","            \"modelo_whisper\": WHISPER_SIZE,\n","            \"modelo_ner\": NER_MODEL,\n","            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n","        }\n","        resultados.append(registro)\n","\n","        # Acumulados\n","        for t, lst in entidades.items():\n","            contador_tipos[t] += len(lst)\n","\n","        # JSON por audio\n","        out_path = os.path.join(OUT_JSON_DIR, f\"{fname}.json\")\n","        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(registro, f, ensure_ascii=False, indent=2)\n","\n","        procesados += 1\n","\n","    except Exception as e:\n","        fallidos += 1\n","        # Guardamos un JSON de error para trazabilidad\n","        registro_err = {\n","            \"archivo_audio\": fname,\n","            \"ruta_audio\": apath,\n","            \"error\": str(e),\n","            \"modelo_whisper\": WHISPER_SIZE,\n","            \"modelo_ner\": NER_MODEL,\n","            \"timestamp\": datetime.utcnow().isoformat() + \"Z\",\n","        }\n","        resultados.append(registro_err)\n","        out_path = os.path.join(OUT_JSON_DIR, f\"{fname}.error.json\")\n","        with open(out_path, \"w\", encoding=\"utf-8\") as f:\n","            json.dump(registro_err, f, ensure_ascii=False, indent=2)\n","\n","# --- JSON global ---\n","GLOBAL_JSON = os.path.join(OUT_JSON_DIR, \"resultados_global.json\")\n","with open(GLOBAL_JSON, \"w\", encoding=\"utf-8\") as f:\n","    json.dump(resultados, f, ensure_ascii=False, indent=2)\n","\n","# --- Resumen en consola ---\n","print(\"Audios totales     :\", len(audio_paths))\n","print(\"Procesados OK      :\", procesados)\n","print(\"Con error          :\", fallidos)\n","print(\"Salida (JSON por audio):\", OUT_JSON_DIR)\n","print(\"JSON global        :\", GLOBAL_JSON)\n","print(\"\\nEntidades por tipo (acumuladas):\")\n","for t, c in sorted(contador_tipos.items(), key=lambda x: (-x[1], x[0])):\n","    print(f\" - {t}: {c}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BpsqIJ_VJ_9K","executionInfo":{"status":"ok","timestamp":1755429347259,"user_tz":-120,"elapsed":174863,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"4ad30f5a-ac16-45db-b2b3-c7d55e56bb1a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 405/405 [00:00<00:00, 1209.52frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 350/350 [00:00<00:00, 2074.54frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 358/358 [00:00<00:00, 857.10frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 417/417 [00:00<00:00, 570.42frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 387/387 [00:00<00:00, 462.65frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 328/328 [00:00<00:00, 835.20frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 368/368 [00:00<00:00, 808.63frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 369/369 [00:00<00:00, 1002.32frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 383/383 [00:00<00:00, 987.69frames/s] \n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 339/339 [00:00<00:00, 526.17frames/s]\n","You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 460/460 [00:00<00:00, 517.99frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 387/387 [00:00<00:00, 2153.78frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 443/443 [00:00<00:00, 1770.22frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 367/367 [00:00<00:00, 1647.97frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 413/413 [00:00<00:00, 1793.31frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 377/377 [00:00<00:00, 1799.79frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 332/332 [00:00<00:00, 1849.88frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 478/478 [00:00<00:00, 1282.39frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 310/310 [00:04<00:00, 64.39frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 340/340 [00:01<00:00, 289.01frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 329/329 [00:00<00:00, 599.86frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 324/324 [00:00<00:00, 1565.33frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 375/375 [00:00<00:00, 1762.12frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 467/467 [00:00<00:00, 2009.04frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 407/407 [00:00<00:00, 1938.56frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 363/363 [00:00<00:00, 1879.74frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 367/367 [00:00<00:00, 1966.52frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 310/310 [00:00<00:00, 1526.75frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 349/349 [00:00<00:00, 1231.33frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 361/361 [00:00<00:00, 2292.13frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 396/396 [00:00<00:00, 1897.78frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 299/299 [00:00<00:00, 1758.72frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 394/394 [00:00<00:00, 1450.89frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 418/418 [00:00<00:00, 1888.58frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 438/438 [00:00<00:00, 658.46frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 407/407 [00:01<00:00, 310.25frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 436/436 [00:00<00:00, 448.92frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 397/397 [00:00<00:00, 793.50frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 382/382 [00:00<00:00, 2950.57frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 389/389 [00:00<00:00, 1864.41frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 384/384 [00:00<00:00, 1778.72frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 430/430 [00:00<00:00, 2335.16frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 340/340 [00:00<00:00, 1794.03frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 358/358 [00:00<00:00, 1429.94frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 280/280 [00:00<00:00, 1139.14frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 310/310 [00:00<00:00, 1735.45frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 332/332 [00:00<00:00, 1637.84frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 394/394 [00:00<00:00, 2009.22frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 355/355 [00:00<00:00, 1738.07frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 489/489 [00:00<00:00, 2441.43frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 386/386 [00:00<00:00, 1926.42frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 319/319 [00:00<00:00, 1711.38frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 406/406 [00:00<00:00, 2015.35frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 410/410 [00:00<00:00, 1662.17frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 449/449 [00:00<00:00, 1424.53frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 303/303 [00:00<00:00, 1673.23frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 386/386 [00:00<00:00, 1977.30frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 417/417 [00:00<00:00, 1651.54frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 426/426 [00:00<00:00, 1998.30frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 411/411 [00:00<00:00, 2105.55frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 435/435 [00:00<00:00, 2366.57frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 452/452 [00:00<00:00, 1607.54frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 425/425 [00:00<00:00, 1236.21frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 348/348 [00:00<00:00, 1913.66frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 422/422 [00:00<00:00, 1891.85frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 391/391 [00:00<00:00, 1783.21frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 360/360 [00:00<00:00, 2116.85frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 376/376 [00:00<00:00, 2124.96frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 355/355 [00:00<00:00, 2163.42frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 397/397 [00:00<00:00, 2012.23frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 425/425 [00:00<00:00, 1925.84frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 310/310 [00:00<00:00, 1133.91frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 376/376 [00:00<00:00, 1467.40frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 415/415 [00:00<00:00, 2120.87frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 424/424 [00:00<00:00, 1959.59frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 340/340 [00:00<00:00, 2042.83frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 400/400 [00:00<00:00, 2023.55frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 429/429 [00:00<00:00, 2568.12frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 481/481 [00:00<00:00, 2131.96frames/s]\n","100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 415/415 [00:00<00:00, 1769.56frames/s]\n"]},{"output_type":"stream","name":"stdout","text":["Audios totales     : 80\n","Procesados OK      : 80\n","Con error          : 0\n","Salida (JSON por audio): /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/outputs/json\n","JSON global        : /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/outputs/json/resultados_global.json\n","\n","Entidades por tipo (acumuladas):\n"," - LOC: 77\n"," - PER: 32\n"," - MISC: 23\n"," - ORG: 23\n"]}]},{"cell_type":"markdown","source":["## 6.Validaci√≥n y resumen de resultados\n","\n","En este apartado revisamos de forma sistem√°tica la calidad del procesamiento por lotes. Para ello:\n","- Leemos el `resultados_global.json` generado en la fase anterior.\n","- Calculamos un resumen por archivo: longitud de la transcripci√≥n y n√∫mero de entidades por tipo (PER, ORG, LOC, MISC) y total.\n","- Generamos un recuento global de las entidades m√°s frecuentes por tipo, con su frecuencia y una confianza media aproximada.\n","- Guardamos dos ficheros de apoyo en `outputs/csv/`: `resumen_transcripciones.csv` (una fila por audio) y `entidades_top.csv` (frecuencias por entidad y tipo).\n","\n","Estos res√∫menes nos permiten detectar r√°pidamente audios con transcripci√≥n vac√≠a o an√≥mala, as√≠ como validar que el etiquetado de entidades es coherente con el contenido."],"metadata":{"id":"WONrCHmYLNes"}},{"cell_type":"code","source":["# Resumen de calidad y guardado de CSV auxiliares.\n","OUT_JSON_DIR = PATHS[\"out_json\"]\n","OUT_CSV_DIR  = PATHS[\"out_csv\"]\n","os.makedirs(OUT_CSV_DIR, exist_ok=True)\n","\n","GLOBAL_JSON = os.path.join(OUT_JSON_DIR, \"resultados_global.json\")\n","\n","# 1) Cargar resultados\n","with open(GLOBAL_JSON, \"r\", encoding=\"utf-8\") as f:\n","    resultados = json.load(f)\n","\n","# 2) Construir resumen por archivo\n","resumen_rows = []\n","vacios = 0\n","tipos_conocidos = (\"PER\", \"ORG\", \"LOC\", \"MISC\")\n","\n","for r in resultados:\n","    fname = r.get(\"archivo_audio\", \"\")\n","    texto = (r.get(\"transcripcion\") or \"\").strip()\n","    ents  = r.get(\"entidades\") or {}\n","    if texto == \"\":\n","        vacios += 1\n","\n","    # Conteos por tipo\n","    counts = {t: len(ents.get(t, [])) for t in tipos_conocidos}\n","    total  = sum(counts.values())\n","\n","    resumen_rows.append({\n","        \"archivo_audio\": fname,\n","        \"longitud_transcripcion\": len(texto),\n","        \"n_PER\": counts[\"PER\"],\n","        \"n_ORG\": counts[\"ORG\"],\n","        \"n_LOC\": counts[\"LOC\"],\n","        \"n_MISC\": counts[\"MISC\"],\n","        \"n_total\": total,\n","    })\n","\n","# Guardar resumen por archivo\n","resumen_csv = os.path.join(OUT_CSV_DIR, \"resumen_transcripciones.csv\")\n","with open(resumen_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n","    writer = csv.DictWriter(\n","        f,\n","        fieldnames=[\n","            \"archivo_audio\", \"longitud_transcripcion\",\n","            \"n_PER\", \"n_ORG\", \"n_LOC\", \"n_MISC\", \"n_total\"\n","        ],\n","    )\n","    writer.writeheader()\n","    writer.writerows(resumen_rows)\n","\n","# 3) Frecuencias de entidades por tipo y texto + confianza media\n","freq = defaultdict(lambda: {\"count\": 0, \"score_sum\": 0.0})\n","for r in resultados:\n","    ents = r.get(\"entidades\") or {}\n","    for t, lista in ents.items():\n","        for e in lista:\n","            clave = (t, e.get(\"texto\", \"\"))\n","            freq[clave][\"count\"] += 1\n","            freq[clave][\"score_sum\"] += float(e.get(\"score\", 0.0))\n","\n","# Preparar filas ordenadas por tipo y frecuencia\n","entidades_rows = []\n","for (t, texto), info in freq.items():\n","    avg_score = info[\"score_sum\"] / max(info[\"count\"], 1)\n","    entidades_rows.append({\n","        \"tipo\": t,\n","        \"texto\": texto,\n","        \"frecuencia\": info[\"count\"],\n","        \"confianza_media\": round(avg_score, 3),\n","    })\n","\n","# Orden: tipo asc, frecuencia desc, texto asc\n","entidades_rows.sort(key=lambda x: (x[\"tipo\"], -x[\"frecuencia\"], x[\"texto\"]))\n","\n","# Guardar CSV de entidades top\n","entidades_csv = os.path.join(OUT_CSV_DIR, \"entidades_top.csv\")\n","with open(entidades_csv, \"w\", encoding=\"utf-8\", newline=\"\") as f:\n","    writer = csv.DictWriter(\n","        f,\n","        fieldnames=[\"tipo\", \"texto\", \"frecuencia\", \"confianza_media\"],\n","    )\n","    writer.writeheader()\n","    writer.writerows(entidades_rows)\n","\n","# 4) Informe corto en consola\n","total = len(resumen_rows)\n","print(\"Audios procesados  :\", total)\n","print(\"Transcripciones vac√≠as:\", vacios)\n","print(\"CSV (resumen por archivo):\", resumen_csv)\n","print(\"CSV (entidades top):      \", entidades_csv)\n","\n","# Mostrar un avance peque√±o (primeras 5 filas del resumen)\n","print(\"\\nPrimeras 5 filas del resumen:\")\n","for row in resumen_rows[:5]:\n","    print(row)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SX2diVLWLON8","executionInfo":{"status":"ok","timestamp":1755429348058,"user_tz":-120,"elapsed":795,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"983c5a3e-064a-4f4b-b00d-5e8ed7b8532a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Audios procesados  : 80\n","Transcripciones vac√≠as: 0\n","CSV (resumen por archivo): /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/outputs/csv/resumen_transcripciones.csv\n","CSV (entidades top):       /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/outputs/csv/entidades_top.csv\n","\n","Primeras 5 filas del resumen:\n","{'archivo_audio': 'frase_01.wav', 'longitud_transcripcion': 61, 'n_PER': 1, 'n_ORG': 0, 'n_LOC': 1, 'n_MISC': 1, 'n_total': 3}\n","{'archivo_audio': 'frase_02.wav', 'longitud_transcripcion': 53, 'n_PER': 1, 'n_ORG': 0, 'n_LOC': 1, 'n_MISC': 0, 'n_total': 2}\n","{'archivo_audio': 'frase_03.wav', 'longitud_transcripcion': 56, 'n_PER': 0, 'n_ORG': 1, 'n_LOC': 1, 'n_MISC': 1, 'n_total': 3}\n","{'archivo_audio': 'frase_04.wav', 'longitud_transcripcion': 54, 'n_PER': 0, 'n_ORG': 0, 'n_LOC': 1, 'n_MISC': 1, 'n_total': 2}\n","{'archivo_audio': 'frase_05.wav', 'longitud_transcripcion': 63, 'n_PER': 0, 'n_ORG': 0, 'n_LOC': 2, 'n_MISC': 0, 'n_total': 2}\n"]}]},{"cell_type":"markdown","source":["## 7.Conclusiones\n","\n","Hemos transcrito todos los audios con Whisper y aplicado NER en espa√±ol con `MMG/xlm-roberta-large-ner-spanish`. Generamos un **JSON por audio** y un **JSON global** con el nombre del archivo, la transcripci√≥n y las entidades agrupadas por tipo, tal y como exige el enunciado. La validaci√≥n final confirma que no hubo errores y que las transcripciones no quedaron vac√≠as.\n","Adem√°s, guardamos dos **CSV de apoyo**:\n","- `resumen_transcripciones.csv`: recoge, para cada audio, la longitud de la transcripci√≥n y el n√∫mero de entidades detectadas por tipo (PER, ORG, LOC, MISC) y el total. Nos sirve para revisar r√°pidamente posibles casos an√≥malos.\n","- `entidades_top.csv`: lista cada entidad detectada (por texto y tipo), su **frecuencia** y la **confianza media**. Nos ayuda a entender qu√© entidades aparecen m√°s y con qu√© seguridad.\n","\n","Como mejoras futuras, podr√≠amos evaluar tama√±os superiores de Whisper para ganar precisi√≥n, revisar entidades etiquetadas como MISC y normalizar nombres propios para an√°lisis posteriores."],"metadata":{"id":"TEX7dna0OMmY"}},{"cell_type":"markdown","source":["## 8.Despliegue con Gradio\n","\n","En este apartado montamos una **mini app web** con **Gradio** para probar el flujo de **transcripci√≥n** (Whisper) y **reconocimiento de entidades** (XLM-R).  \n","La interfaz permite subir un audio (o grabarlo con el micro), lanzar la transcripci√≥n y visualizar una tabla con las entidades detectadas.\n","\n","**Objetivos:**\n","- Facilitar la **demostraci√≥n** del modelo a no t√©cnicos.\n","- Generar **evidencias** (JSON por ejecuci√≥n) sin reentrenar ni alterar el pipeline.\n","\n","**Metodolog√≠a y banderas:**\n","- Respetamos `USE_DRIVE` para rutas.\n","- A√±adimos `SAVE_RESULTS` para **guardar** o no las salidas de la app (por defecto, activado).\n","- Si los modelos (`whisper_model`, `ner_pipe`) est√°n ya en memoria, **se reutilizan**; en caso contrario, se **cargan** con la misma configuraci√≥n del cuaderno.\n","\n","**Salidas (si `SAVE_RESULTS=True`):**\n","- Un JSON por ejecuci√≥n en `outputs/json/gradio_runs/` con transcripci√≥n, entidades y metadatos.\n","- Guardamos las capturas de pantalla en `outputs/figures/`\n","\n","> Preparamos el despliegue con Gradio de forma aut√≥noma, de modo que puede ejecutarse sin necesidad de rehacer todos los pasos previos pero manteniendo la coherencia con los modelos utilizados en el ejercicio.\n","> Est√° optimizado para su ejecuci√≥n en Google Colab.\n","> De esta forma se genera autom√°ticamente un enlace p√∫blico temporal (`.gradio.live`) accesible desde cualquier navegador.\n","> En un entorno local (Jupyter Notebook), bastar√≠a con instalar Gradio y ejecutar las mismas celdas para abrir la interfaz en `http://127.0.0.1:7860`."],"metadata":{"id":"hme0ggVIlJq1"}},{"cell_type":"code","source":["# Flag para controlar si lanzamos realmente el despliegue o solo cargamos resultados ya generados\n","RUN_DEPLOY = True"],"metadata":{"id":"J2gU2VnZrrH9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# === Configuraci√≥n para el despliegue con Gradio ===\n","from pathlib import Path\n","import json, os\n","from datetime import datetime\n","\n","# Directorios\n","OUT_JSON_DIR = Path(PATHS[\"out_json\"])\n","GRADIO_RUNS  = OUT_JSON_DIR / \"gradio_runs\"\n","GRADIO_RUNS.mkdir(parents=True, exist_ok=True)\n","\n","# Seleccionamos una muestra real para el modo entregable\n","candidatos = sorted(GRADIO_RUNS.glob(\"*.json\"))\n","if len(candidatos) == 0:\n","    ejemplo = {\n","        \"archivo_audio\": \"ejemplo.wav\",\n","        \"transcripcion\": \"Buenos d√≠as, esto es un ejemplo de transcripci√≥n.\",\n","        \"entidades\": {\"LOC\": [{\"texto\": \"Madrid\", \"score\": 0.98}]},\n","        \"modelo_whisper\": \"base\",\n","        \"modelo_ner\": \"MMG/xlm-roberta-large-ner-spanish\",\n","        \"timestamp\": datetime.utcnow().isoformat()+\"Z\",\n","    }\n","    sample_path = GRADIO_RUNS / \"sample_gradio.json\"\n","    with open(sample_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(ejemplo, f, ensure_ascii=False, indent=2)\n","    print(\"‚ÑπÔ∏è No se encontraron JSON previos. Creado ejemplo sint√©tico en:\", sample_path)\n","else:\n","    src = candidatos[0]\n","    sample_path = GRADIO_RUNS / \"sample_gradio.json\"\n","    with open(src, \"r\", encoding=\"utf-8\") as f:\n","        data = json.load(f)\n","    ejemplo = {\n","        \"archivo_audio\": data.get(\"archivo_audio\", os.path.basename(data.get(\"ruta_audio\",\"muestra.wav\"))),\n","        \"transcripcion\": data.get(\"transcripcion\", data.get(\"texto\",\"\")),\n","        \"entidades\": data.get(\"entidades\", {}),\n","        \"modelo_whisper\": \"base\",\n","        \"modelo_ner\": \"MMG/xlm-roberta-large-ner-spanish\",\n","        \"timestamp\": datetime.utcnow().isoformat()+\"Z\",\n","    }\n","    with open(sample_path, \"w\", encoding=\"utf-8\") as f:\n","        json.dump(ejemplo, f, ensure_ascii=False, indent=2)\n","    print(\"‚úÖ Muestra real preparada para el modo entregable:\", sample_path)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mInWhdjbmyqt","executionInfo":{"status":"ok","timestamp":1755430208510,"user_tz":-120,"elapsed":34,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"ef693f4d-a1dc-4203-9bc6-ab6592562028"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["‚úÖ Muestra real preparada para el modo entregable: /content/drive/MyDrive/MASTER BIG DATA/md2_2025/ej1_transcripcion/outputs/json/gradio_runs/sample_gradio.json\n"]}]},{"cell_type":"code","source":["# === 8.B) Despliegue con Gradio (limpio, integrado con PATHS/GRADIO_RUNS) ===\n","# Usa RUN_DEPLOY:\n","#   - True  ‚Üí instala condicionalmente, carga modelos y lanza Gradio (en Colab: share=True)\n","#   - False ‚Üí modo entregable: muestra la muestra real 'sample_gradio.json' (sin instalar nada)\n","\n","# Comprobaci√≥n por si RUN_DEPLOY no existe:\n","if \"RUN_DEPLOY\" not in globals():\n","    RUN_DEPLOY = False\n","\n","if RUN_DEPLOY:\n","    # --- Instalaci√≥n condicional de dependencias pesadas ---\n","    try:\n","        import whisper\n","    except ImportError:\n","        %pip -q install -U openai-whisper\n","        import whisper\n","\n","    try:\n","        import gradio as gr\n","    except ImportError:\n","        %pip -q install -U gradio\n","        import gradio as gr\n","\n","    try:\n","        import transformers\n","        from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n","    except ImportError:\n","        %pip -q install -U transformers accelerate\n","        from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n","\n","    import shutil\n","    if not shutil.which(\"ffmpeg\"):\n","        !apt -q install -y ffmpeg\n","\n","    # --- Imports espec√≠ficos para el despliegue ---\n","    import torch\n","    import pandas as pd\n","\n","    # --- Config coherente con pipeline (si existen, reusa; si no, default) ---\n","    device       = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","    WHISPER_SIZE = globals().get(\"WHISPER_SIZE\", \"base\")\n","    NER_MODEL    = globals().get(\"NER_MODEL\", \"MMG/xlm-roberta-large-ner-spanish\")\n","\n","    # --- Carga perezosa de modelos (si ya estaban en memoria, se reutilizan) ---\n","    if \"whisper_model\" not in globals():\n","        whisper_model = whisper.load_model(WHISPER_SIZE, device=device)\n","\n","    if \"ner_pipe\" not in globals():\n","        tokenizer = AutoTokenizer.from_pretrained(NER_MODEL)\n","        ner_model = AutoModelForTokenClassification.from_pretrained(NER_MODEL)\n","        ner_pipe  = pipeline(\n","            \"token-classification\",\n","            model=ner_model,\n","            tokenizer=tokenizer,\n","            aggregation_strategy=\"simple\",\n","            device=0 if device == \"cuda\" else -1\n","        )\n","\n","    # --- Auxiliar: convertir entidades a DataFrame legible ---\n","    def _ents_to_df(ents):\n","        if not ents:\n","            return pd.DataFrame(columns=[\"entity_group\",\"word\",\"score\",\"start\",\"end\"])\n","        rows = []\n","        for e in ents:\n","            rows.append({\n","                \"entity_group\": e.get(\"entity_group\",\"\"),\n","                \"word\": e.get(\"word\",\"\"),\n","                \"score\": float(e.get(\"score\",0.0)),\n","                \"start\": e.get(\"start\", None),\n","                \"end\": e.get(\"end\", None),\n","            })\n","        return pd.DataFrame(rows)[[\"entity_group\",\"word\",\"score\",\"start\",\"end\"]]\n","\n","    # --- L√≥gica de la app: transcribe y hace NER; guarda JSON por ejecuci√≥n ---\n","    def transcribir_y_ner(audio_path: str, language: str = \"es\"):\n","        if not audio_path or not os.path.exists(audio_path):\n","            return \"‚ö†Ô∏è No se recibi√≥ audio.\", pd.DataFrame(columns=[\"entity_group\",\"word\",\"score\",\"start\",\"end\"])\n","\n","        # 1) Transcripci√≥n\n","        result = whisper_model.transcribe(audio_path, language=language, verbose=False)\n","        texto  = (result.get(\"text\") or \"\").strip()\n","\n","        # 2) NER\n","        ents   = ner_pipe(texto) if texto else []\n","        df     = _ents_to_df(ents)\n","\n","        # 3) Guardado JSON de esta ejecuci√≥n en outputs/json/gradio_runs\n","        payload = {\n","            \"timestamp_utc\": datetime.utcnow().isoformat()+\"Z\",\n","            \"audio_file\": os.path.basename(audio_path),\n","            \"language\": language,\n","            \"whisper_size\": WHISPER_SIZE,\n","            \"ner_model\": NER_MODEL,\n","            \"transcripcion\": texto,\n","            \"entidades\": df.to_dict(orient=\"records\"),\n","        }\n","        run_name = f\"gradio_run_{datetime.utcnow().strftime('%Y%m%dT%H%M%SZ')}.json\"\n","        with open((Path(PATHS[\"out_json\"]) / \"gradio_runs\" / run_name), \"w\", encoding=\"utf-8\") as f:\n","            json.dump(payload, f, ensure_ascii=False, indent=2)\n","\n","        return texto, df\n","\n","    # --- Interfaz Gradio ---\n","    with gr.Blocks(title=\"Transcripci√≥n + NER (Whisper + XLM-R)\") as demo:\n","        gr.Markdown(\"## üéôÔ∏è Transcripci√≥n y entidades\\nSube un audio en espa√±ol y obt√©n el texto y las entidades extra√≠das.\")\n","        with gr.Row():\n","            audio_in = gr.Audio(sources=[\"upload\",\"microphone\"], type=\"filepath\", label=\"Audio (wav/mp3/m4a)\")\n","            lang_in  = gr.Dropdown(choices=[\"es\",\"en\",\"fr\",\"de\",\"it\",\"pt\"], value=\"es\", label=\"Idioma\")\n","        btn     = gr.Button(\"Transcribir\")\n","        txt_out = gr.Textbox(label=\"Transcripci√≥n\", lines=8)\n","        ents_df = gr.Dataframe(label=\"Entidades (NER)\", interactive=False)\n","\n","        btn.click(transcribir_y_ner, inputs=[audio_in, lang_in], outputs=[txt_out, ents_df])\n","\n","    # En Colab, share=True para URL p√∫blica temporal (.gradio.live)\n","    demo.launch(share=True)\n","\n","else:\n","    # === MODO ENTREGABLE: muestra sample_gradio.json creado en 8.A (sin instalar nada) ===\n","    sample_path = Path(PATHS[\"out_json\"]) / \"gradio_runs\" / \"sample_gradio.json\"\n","    if sample_path.exists():\n","        with open(sample_path, \"r\", encoding=\"utf-8\") as f:\n","            data = json.load(f)\n","        print(\"üìÑ Ejemplo real (modo entregable):\")\n","        print(json.dumps(data, ensure_ascii=False, indent=2))\n","    else:\n","        print(\"‚ö†Ô∏è No se encontr√≥ sample_gradio.json en\", sample_path.parent)\n"],"metadata":{"id":"MgHOVCkM5XS2","colab":{"base_uri":"https://localhost:8080/","height":612},"executionInfo":{"status":"ok","timestamp":1755430877852,"user_tz":-120,"elapsed":2427,"user":{"displayName":"Oscar Porta Serra","userId":"17017378231525430001"}},"outputId":"10e9e245-2abd-46a2-e2a0-3c80dfa3127e"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n","* Running on public URL: https://124c5cf105481c87e6.gradio.live\n","\n","This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"]},{"output_type":"display_data","data":{"text/plain":["<IPython.core.display.HTML object>"],"text/html":["<div><iframe src=\"https://124c5cf105481c87e6.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"]},"metadata":{}}]},{"cell_type":"code","source":[],"metadata":{"id":"omAU3Z-6pOQt"},"execution_count":null,"outputs":[]}]}